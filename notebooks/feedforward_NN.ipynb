{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": True
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": False
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ann_dataset1.tar\n",
      "http://mrtee.europa.renci.org/~bblanton/ANN/ann_dataset1.tar\n",
      "Found and verified ann_dataset1.tar\n"
     ]
    }
   ],
   "source": [
    "# Download and save the archived data\n",
    "\n",
    "url = 'http://mrtee.europa.renci.org/~bblanton/ANN/'\n",
    "to = \"../data\"\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    print(os.path.join(to,filename))\n",
    "    print(url+filename)\n",
    "    if force or not os.path.exists(os.path.join(to,filename)):\n",
    "        filename, _ = urlretrieve(url + filename, os.path.join(to,filename))\n",
    "    statinfo = os.stat(os.path.join(to,filename))\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        raise Exception(\n",
    "          'Failed to verify' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "data_filename = maybe_download('ann_dataset1.tar', 5642240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": False
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data already present - Skipping extraction of ../data/ann_dataset1.tar.\n"
     ]
    }
   ],
   "source": [
    "# Extract files from the archive\n",
    "def maybe_extract(filename, force=False):\n",
    "    extract_folder = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    root = os.path.dirname(filename)\n",
    "    if os.path.isdir(extract_folder) and not force:\n",
    "    # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(path = root)\n",
    "        tar.close()\n",
    "    data_files = [\n",
    "        os.path.join(extract_folder, d) for d in sorted(os.listdir(extract_folder))\n",
    "        if os.path.isdir(extract_folder)]\n",
    "    return data_files\n",
    "  \n",
    "data_filename = \"../data/ann_dataset1.tar\"\n",
    "data_files = maybe_extract(data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": False,
    "scrolled": True
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/324 \n",
      "\n",
      "Processed 100/324 \n",
      "\n",
      "Processed 200/324 \n",
      "\n",
      "Processed 300/324 \n",
      "\n",
      "(324, 193, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load files and produce dataset\n",
    "def maybe_load(file_names):\n",
    "    names = ('index','time', 'long', 'lat', 'param1', 'param2', 'param3', 'param4', 'out1', 'out2')\n",
    "    datafile_length = 193\n",
    "    dataset = np.ndarray(shape=(len(file_names), datafile_length, len(names)))\n",
    "    for i in range(0,len(file_names)):\n",
    "        a = np.loadtxt(file_names[i])\n",
    "        a = np.asarray([x for xs in a for x in xs],dtype='d').reshape([datafile_length,len(names)])\n",
    "        dataset[i,:,:] = a\n",
    "        if i%100 == 0:\n",
    "            print(\"Processed %d/%d \\n\"%(i,len(file_names)))\n",
    "    return dataset\n",
    "\n",
    "dataset = maybe_load(data_files)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": False
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset: (48, 193, 10)\n",
      "Validation dataset: (49, 193, 10)\n",
      "Training dataset: (227, 193, 10)\n"
     ]
    }
   ],
   "source": [
    "# train, validation, and test dataset percentages\n",
    "train_percent = 70\n",
    "valid_percent = 15\n",
    "test_percent = 15\n",
    "\n",
    "# train, validation, and test dataset indices\n",
    "# test: test_start : valid_start-1\n",
    "# validation: valid_start : train_start-1\n",
    "# training: train_start : dataset.shape[0]\n",
    "test_start = 0 \n",
    "valid_start = int(test_percent/100.0*dataset.shape[0])\n",
    "train_start = int((test_percent+valid_percent)/100.0*dataset.shape[0])\n",
    "\n",
    "# Shuffle file indices\n",
    "file_indices = range(dataset.shape[0])\n",
    "np.random.shuffle(file_indices)\n",
    "\n",
    "# Assign datasets\n",
    "test_dataset = np.array([dataset[j,:,:] for j in [file_indices[i] for i in range(test_start, valid_start)]])\n",
    "valid_dataset = np.array([dataset[j,:,:] for j in [file_indices[i] for i in range(valid_start, train_start)]])\n",
    "train_dataset = np.array([dataset[j,:,:] for j in [file_indices[i] for i in range(train_start, dataset.shape[0])]])\n",
    "\n",
    "# Save memory\n",
    "#del(dataset)\n",
    "print(\"Test dataset: \"+str(test_dataset.shape))\n",
    "print(\"Validation dataset: \"+str(valid_dataset.shape))\n",
    "print(\"Training dataset: \"+str(train_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": False
   },
   "outputs": [],
   "source": [
    "def accuracy_mse(predictions, outputs):\n",
    "    err = predictions-outputs\n",
    "    return np.mean(err*err)\n",
    "\n",
    "def Normalize(x, means, stds):\n",
    "    return (x-means)/stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": False
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43811, 6)\n",
      "[[-0.25979745 -0.0609056 ]\n",
      " [-0.26086986 -0.05793402]\n",
      " [-0.26044089 -0.05867691]\n",
      " ..., \n",
      " [-0.59438699 -0.40857971]\n",
      " [-0.58409196 -0.41898024]\n",
      " [-0.57658511 -0.4338381 ]]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data and normalize\n",
    "\n",
    "train_dataset2 = train_dataset[:,:,1:7].reshape((-1, 6)).astype(np.float32)\n",
    "train_output = train_dataset[:,:,8:10].reshape((-1, 2)).astype(np.float32)\n",
    "\n",
    "# calculate means and stds for training dataset\n",
    "input_means = [np.mean(train_dataset2[:,i]) for i in range(train_dataset2.shape[1])]\n",
    "input_stds = [np.std(train_dataset2[:,i]) for i in range(train_dataset2.shape[1])]\n",
    "output_means = [np.mean(train_output[:,i]) for i in range(train_output.shape[1])]\n",
    "output_stds = [np.std(train_output[:,i]) for i in range(train_output.shape[1])]\n",
    "\n",
    "train_dataset2 = Normalize(train_dataset2, input_means, input_stds)\n",
    "train_output = Normalize(train_output, output_means, output_stds)\n",
    "\n",
    "print(train_dataset2.shape)\n",
    "print(train_output)\n",
    "\n",
    "plt.plot(train_dataset2[:193,:],label=\"input\")\n",
    "plt.plot(train_output[:193,:],label=\"output\")\n",
    "plt.ylabel(\"training data\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()\n",
    "\n",
    "valid_dataset2 = Normalize(valid_dataset[:,:,1:7].reshape((-1, 6)).astype(np.float32), input_means, input_stds)\n",
    "valid_output = Normalize(valid_dataset[:,:,8:10].reshape((-1, 2)).astype(np.float32), output_means, output_stds)\n",
    "\n",
    "test_dataset2 = Normalize(test_dataset[:,:,1:7].reshape((-1, 6)).astype(np.float32),input_means, input_stds)\n",
    "test_output = Normalize(test_dataset[:,:,8:10].reshape((-1, 2)).astype(np.float32), output_means, output_stds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": True
   },
   "outputs": [],
   "source": [
    "class DoubleGDOptimizer(tf.train.GradientDescentOptimizer):\n",
    "  def _valid_dtypes(self):\n",
    "    return set([tf.float32, tf.float64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": False
   },
   "outputs": [],
   "source": [
    "# Deep ANN\n",
    "batch_size = 10*193\n",
    "hidden_nodes_1 = 25\n",
    "hidden_nodes_2 = 10\n",
    "hidden_nodes_3 = 6\n",
    "\n",
    "num_steps = 300001\n",
    "starter_learning_rate = 0.03\n",
    "#starter_learning_rate = 0.005\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 6)) #train_dataset2.shape(2)\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 2))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset2)\n",
    "    tf_test_dataset = tf.constant(test_dataset2)\n",
    "  \n",
    "    weights_0 = tf.Variable(tf.truncated_normal([6,hidden_nodes_1], dtype = tf.float32))\n",
    "    biases_0 = tf.Variable(tf.zeros([hidden_nodes_1], dtype = tf.float32))\n",
    "    \n",
    "    weights_1 = tf.Variable(tf.truncated_normal([hidden_nodes_1,hidden_nodes_2], dtype = tf.float32))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes_2], dtype = tf.float32))\n",
    "    \n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_2,2], dtype = tf.float32))\n",
    "    biases_2 = tf.Variable(tf.zeros([2], dtype = tf.float32))\n",
    "\n",
    "  \n",
    "    input_layer_output = tf.sigmoid(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "    hidden_layer_output = tf.sigmoid(tf.matmul(input_layer_output, weights_1) + biases_1)\n",
    "    #hidden_layer_output = tf.nn.dropout(hidden_layer_output, 0.5)\n",
    "    hidden_layer_output = tf.matmul(hidden_layer_output, weights_2) + biases_2\n",
    "    \n",
    "    \n",
    "    loss = tf.cast(tf.reduce_mean(tf.reduce_mean(tf.square(hidden_layer_output-tf_train_labels))),tf.float32)\n",
    "    #loss = tf.cast(tf.reduce_mean(tf.reduce_mean(tf.square(tf.square(hidden_layer_output-tf_train_labels)))),tf.float32)\n",
    "        \n",
    "    global_step = tf.Variable(0.00, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, num_steps, 0.96, staircase=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #starter_learning_rate = 0.5\n",
    "    #optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "    train_prediction = loss\n",
    "    valid_prediction = tf.sigmoid(tf.matmul(tf_valid_dataset, weights_0) + biases_0)\n",
    "    valid_prediction = tf.sigmoid(tf.matmul(valid_prediction, weights_1) + biases_1)\n",
    "    valid_prediction = tf.matmul(valid_prediction, weights_2) + biases_2\n",
    "    \n",
    "    test_prediction = tf.sigmoid(tf.matmul(tf_test_dataset, weights_0) + biases_0)\n",
    "    test_prediction = tf.sigmoid(tf.matmul(test_prediction, weights_1) + biases_1)\n",
    "    test_prediction = tf.matmul(test_prediction, weights_2) + biases_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": False,
    "scrolled": True
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 3.981769\n",
      "Validation at step 0 MSE: 3.2842\n",
      "Loss at step 1000: 0.709574\n",
      "Loss at step 2000: 0.732000\n",
      "Loss at step 3000: 0.385466\n",
      "Loss at step 4000: 0.762925\n",
      "Loss at step 5000: 0.562984\n",
      "Validation at step 5000 MSE: 0.4196\n",
      "Loss at step 6000: 0.380336\n",
      "Loss at step 7000: 0.380473\n",
      "Loss at step 8000: 0.275024\n",
      "Loss at step 9000: 0.286791\n",
      "Loss at step 10000: 0.334317\n",
      "Validation at step 10000 MSE: 0.3281\n",
      "Loss at step 11000: 0.328688\n",
      "Loss at step 12000: 0.210568\n",
      "Loss at step 13000: 0.295228\n",
      "Loss at step 14000: 0.378103\n",
      "Loss at step 15000: 0.178257\n",
      "Validation at step 15000 MSE: 0.2760\n",
      "Loss at step 16000: 0.390443\n",
      "Loss at step 17000: 0.156740\n",
      "Loss at step 18000: 0.240932\n",
      "Loss at step 19000: 0.202357\n",
      "Loss at step 20000: 0.180823\n",
      "Validation at step 20000 MSE: 0.2513\n",
      "Loss at step 21000: 0.272829\n",
      "Loss at step 22000: 0.172295\n",
      "Loss at step 23000: 0.154696\n",
      "Loss at step 24000: 0.122746\n",
      "Loss at step 25000: 0.225848\n",
      "Validation at step 25000 MSE: 0.2326\n",
      "Loss at step 26000: 0.245020\n",
      "Loss at step 27000: 0.147936\n",
      "Loss at step 28000: 0.256206\n",
      "Loss at step 29000: 0.120872\n",
      "Loss at step 30000: 0.207567\n",
      "Validation at step 30000 MSE: 0.2255\n",
      "Loss at step 31000: 0.192918\n",
      "Loss at step 32000: 0.119370\n",
      "Loss at step 33000: 0.215291\n",
      "Loss at step 34000: 0.121427\n",
      "Loss at step 35000: 0.117836\n",
      "Validation at step 35000 MSE: 0.2174\n",
      "Loss at step 36000: 0.110537\n",
      "Loss at step 37000: 0.194969\n",
      "Loss at step 38000: 0.204065\n",
      "Loss at step 39000: 0.124893\n",
      "Loss at step 40000: 0.204936\n",
      "Validation at step 40000 MSE: 0.2112\n",
      "Loss at step 41000: 0.099290\n",
      "Loss at step 42000: 0.191519\n",
      "Loss at step 43000: 0.183483\n",
      "Loss at step 44000: 0.103986\n",
      "Loss at step 45000: 0.180083\n",
      "Validation at step 45000 MSE: 0.2064\n",
      "Loss at step 46000: 0.108764\n",
      "Loss at step 47000: 0.122964\n",
      "Loss at step 48000: 0.101621\n",
      "Loss at step 49000: 0.158068\n",
      "Loss at step 50000: 0.185233\n",
      "Validation at step 50000 MSE: 0.2010\n",
      "Loss at step 51000: 0.137247\n",
      "Loss at step 52000: 0.159069\n",
      "Loss at step 53000: 0.089029\n",
      "Loss at step 54000: 0.222155\n",
      "Loss at step 55000: 0.119659\n",
      "Validation at step 55000 MSE: 0.1976\n",
      "Loss at step 56000: 0.093817\n",
      "Loss at step 57000: 0.171942\n",
      "Loss at step 58000: 0.081164\n",
      "Loss at step 59000: 0.114339\n",
      "Loss at step 60000: 0.074937\n",
      "Validation at step 60000 MSE: 0.1939\n",
      "Loss at step 61000: 0.151148\n",
      "Loss at step 62000: 0.163827\n",
      "Loss at step 63000: 0.118149\n",
      "Loss at step 64000: 0.158567\n",
      "Loss at step 65000: 0.098386\n",
      "Validation at step 65000 MSE: 0.1916\n",
      "Loss at step 66000: 0.189493\n",
      "Loss at step 67000: 0.107283\n",
      "Loss at step 68000: 0.090422\n",
      "Loss at step 69000: 0.154738\n",
      "Loss at step 70000: 0.106256\n",
      "Validation at step 70000 MSE: 0.1884\n",
      "Loss at step 71000: 0.110497\n",
      "Loss at step 72000: 0.093669\n",
      "Loss at step 73000: 0.152269\n",
      "Loss at step 74000: 0.142036\n",
      "Loss at step 75000: 0.113995\n",
      "Validation at step 75000 MSE: 0.1852\n",
      "Loss at step 76000: 0.154493\n",
      "Loss at step 77000: 0.110760\n",
      "Loss at step 78000: 0.187631\n",
      "Loss at step 79000: 0.090826\n",
      "Loss at step 80000: 0.083212\n",
      "Validation at step 80000 MSE: 0.1843\n",
      "Loss at step 81000: 0.142595\n",
      "Loss at step 82000: 0.106478\n",
      "Loss at step 83000: 0.108584\n",
      "Loss at step 84000: 0.097935\n",
      "Loss at step 85000: 0.139982\n",
      "Validation at step 85000 MSE: 0.1814\n",
      "Loss at step 86000: 0.131344\n",
      "Loss at step 87000: 0.121595\n",
      "Loss at step 88000: 0.152080\n",
      "Loss at step 89000: 0.089632\n",
      "Loss at step 90000: 0.197500\n",
      "Validation at step 90000 MSE: 0.1806\n",
      "Loss at step 91000: 0.086440\n",
      "Loss at step 92000: 0.079328\n",
      "Loss at step 93000: 0.140697\n",
      "Loss at step 94000: 0.123977\n",
      "Loss at step 95000: 0.119589\n",
      "Validation at step 95000 MSE: 0.1789\n",
      "Loss at step 96000: 0.077543\n",
      "Loss at step 97000: 0.083945\n",
      "Loss at step 98000: 0.083808\n",
      "Loss at step 99000: 0.111161\n",
      "Loss at step 100000: 0.140152\n",
      "Validation at step 100000 MSE: 0.1764\n",
      "Loss at step 101000: 0.085265\n",
      "Loss at step 102000: 0.189017\n",
      "Loss at step 103000: 0.071902\n",
      "Loss at step 104000: 0.073717\n",
      "Loss at step 105000: 0.154324\n",
      "Validation at step 105000 MSE: 0.1752\n",
      "Loss at step 106000: 0.130063\n",
      "Loss at step 107000: 0.141770\n",
      "Loss at step 108000: 0.081832\n",
      "Loss at step 109000: 0.096329\n",
      "Loss at step 110000: 0.076232\n",
      "Validation at step 110000 MSE: 0.1731\n",
      "Loss at step 111000: 0.119030\n",
      "Loss at step 112000: 0.099946\n",
      "Loss at step 113000: 0.087243\n",
      "Loss at step 114000: 0.199510\n",
      "Loss at step 115000: 0.100801\n",
      "Validation at step 115000 MSE: 0.1726\n",
      "Loss at step 116000: 0.066184\n",
      "Loss at step 117000: 0.146202\n",
      "Loss at step 118000: 0.134693\n",
      "Loss at step 119000: 0.127116\n",
      "Loss at step 120000: 0.131571\n",
      "Validation at step 120000 MSE: 0.1707\n",
      "Loss at step 121000: 0.089179\n",
      "Loss at step 122000: 0.073983\n",
      "Loss at step 123000: 0.112131\n",
      "Loss at step 124000: 0.107483\n",
      "Loss at step 125000: 0.084845\n",
      "Validation at step 125000 MSE: 0.1700\n",
      "Loss at step 126000: 0.177101\n",
      "Loss at step 127000: 0.107003\n",
      "Loss at step 128000: 0.062975\n",
      "Loss at step 129000: 0.150338\n",
      "Loss at step 130000: 0.126446\n",
      "Validation at step 130000 MSE: 0.1692\n",
      "Loss at step 131000: 0.122105\n",
      "Loss at step 132000: 0.147299\n",
      "Loss at step 133000: 0.092196\n",
      "Loss at step 134000: 0.073774\n",
      "Loss at step 135000: 0.108501\n",
      "Validation at step 135000 MSE: 0.1676\n",
      "Loss at step 136000: 0.112680\n",
      "Loss at step 137000: 0.082743\n",
      "Loss at step 138000: 0.166633\n",
      "Loss at step 139000: 0.102385\n",
      "Loss at step 140000: 0.059650\n",
      "Validation at step 140000 MSE: 0.1676\n",
      "Loss at step 141000: 0.075860\n",
      "Loss at step 142000: 0.131880\n",
      "Loss at step 143000: 0.091083\n",
      "Loss at step 144000: 0.143428\n",
      "Loss at step 145000: 0.084653\n",
      "Validation at step 145000 MSE: 0.1657\n",
      "Loss at step 146000: 0.092996\n",
      "Loss at step 147000: 0.099190\n",
      "Loss at step 148000: 0.107575\n",
      "Loss at step 149000: 0.087998\n",
      "Loss at step 150000: 0.153382\n",
      "Validation at step 150000 MSE: 0.1669\n",
      "Loss at step 151000: 0.098556\n",
      "Loss at step 152000: 0.092658\n",
      "Loss at step 153000: 0.071031\n",
      "Loss at step 154000: 0.138072\n",
      "Loss at step 155000: 0.086390\n",
      "Validation at step 155000 MSE: 0.1660\n",
      "Loss at step 156000: 0.136354\n",
      "Loss at step 157000: 0.082982\n",
      "Loss at step 158000: 0.077536\n",
      "Loss at step 159000: 0.103258\n",
      "Loss at step 160000: 0.112614\n",
      "Validation at step 160000 MSE: 0.1645\n",
      "Loss at step 161000: 0.089932\n",
      "Loss at step 162000: 0.142823\n",
      "Loss at step 163000: 0.097689\n",
      "Loss at step 164000: 0.093350\n",
      "Loss at step 165000: 0.067975\n",
      "Validation at step 165000 MSE: 0.1642\n",
      "Loss at step 166000: 0.138115\n",
      "Loss at step 167000: 0.073356\n",
      "Loss at step 168000: 0.135248\n",
      "Loss at step 169000: 0.083944\n",
      "Loss at step 170000: 0.074931\n",
      "Validation at step 170000 MSE: 0.1628\n",
      "Loss at step 171000: 0.081477\n",
      "Loss at step 172000: 0.107359\n",
      "Loss at step 173000: 0.105877\n",
      "Loss at step 174000: 0.096155\n",
      "Loss at step 175000: 0.094518\n",
      "Validation at step 175000 MSE: 0.1632\n",
      "Loss at step 176000: 0.105066\n",
      "Loss at step 177000: 0.067051\n",
      "Loss at step 178000: 0.141441\n",
      "Loss at step 179000: 0.072208\n",
      "Loss at step 180000: 0.132292\n",
      "Validation at step 180000 MSE: 0.1618\n",
      "Loss at step 181000: 0.098813\n",
      "Loss at step 182000: 0.070617\n",
      "Loss at step 183000: 0.081488\n",
      "Loss at step 184000: 0.096899\n",
      "Loss at step 185000: 0.104181\n",
      "Validation at step 185000 MSE: 0.1622\n",
      "Loss at step 186000: 0.105177\n",
      "Loss at step 187000: 0.106028\n",
      "Loss at step 188000: 0.099232\n",
      "Loss at step 189000: 0.067643\n",
      "Loss at step 190000: 0.114642\n",
      "Validation at step 190000 MSE: 0.1613\n",
      "Loss at step 191000: 0.069062\n",
      "Loss at step 192000: 0.111477\n",
      "Loss at step 193000: 0.097508\n",
      "Loss at step 194000: 0.101576\n",
      "Loss at step 195000: 0.116052\n",
      "Validation at step 195000 MSE: 0.1609\n",
      "Loss at step 196000: 0.078014\n",
      "Loss at step 197000: 0.096321\n",
      "Loss at step 198000: 0.095604\n",
      "Loss at step 199000: 0.100965\n",
      "Loss at step 200000: 0.095109\n",
      "Validation at step 200000 MSE: 0.1605\n",
      "Loss at step 201000: 0.063273\n",
      "Loss at step 202000: 0.114109\n",
      "Loss at step 203000: 0.083001\n",
      "Loss at step 204000: 0.102439\n",
      "Loss at step 205000: 0.095252\n",
      "Validation at step 205000 MSE: 0.1594\n",
      "Loss at step 206000: 0.096539\n",
      "Loss at step 207000: 0.110347\n",
      "Loss at step 208000: 0.057879\n",
      "Loss at step 209000: 0.125667\n",
      "Loss at step 210000: 0.082488\n",
      "Validation at step 210000 MSE: 0.1600\n",
      "Loss at step 211000: 0.110397\n",
      "Loss at step 212000: 0.093642\n",
      "Loss at step 213000: 0.066897\n",
      "Loss at step 214000: 0.100256\n",
      "Loss at step 215000: 0.072078\n",
      "Validation at step 215000 MSE: 0.1595\n",
      "Loss at step 216000: 0.100773\n",
      "Loss at step 217000: 0.123053\n",
      "Loss at step 218000: 0.098163\n",
      "Loss at step 219000: 0.105325\n",
      "Loss at step 220000: 0.065054\n",
      "Validation at step 220000 MSE: 0.1586\n",
      "Loss at step 221000: 0.128402\n",
      "Loss at step 222000: 0.086922\n",
      "Loss at step 223000: 0.129864\n",
      "Loss at step 224000: 0.090313\n",
      "Loss at step 225000: 0.056861\n",
      "Validation at step 225000 MSE: 0.1586\n",
      "Loss at step 226000: 0.073964\n",
      "Loss at step 227000: 0.056606\n",
      "Loss at step 228000: 0.094975\n",
      "Loss at step 229000: 0.066905\n",
      "Loss at step 230000: 0.111191\n",
      "Validation at step 230000 MSE: 0.1578\n",
      "Loss at step 231000: 0.094179\n",
      "Loss at step 232000: 0.066965\n",
      "Loss at step 233000: 0.126999\n",
      "Loss at step 234000: 0.067062\n",
      "Loss at step 235000: 0.105219\n",
      "Validation at step 235000 MSE: 0.1587\n",
      "Loss at step 236000: 0.100475\n",
      "Loss at step 237000: 0.069385\n",
      "Loss at step 238000: 0.121364\n",
      "Loss at step 239000: 0.058490\n",
      "Loss at step 240000: 0.067496\n",
      "Validation at step 240000 MSE: 0.1578\n",
      "Loss at step 241000: 0.062068\n",
      "Loss at step 242000: 0.118060\n",
      "Loss at step 243000: 0.090164\n",
      "Loss at step 244000: 0.070398\n",
      "Loss at step 245000: 0.130797\n",
      "Validation at step 245000 MSE: 0.1577\n",
      "Loss at step 246000: 0.073846\n",
      "Loss at step 247000: 0.113452\n",
      "Loss at step 248000: 0.100810\n",
      "Loss at step 249000: 0.056101\n",
      "Loss at step 250000: 0.119172\n",
      "Validation at step 250000 MSE: 0.1565\n",
      "Loss at step 251000: 0.056990\n",
      "Loss at step 252000: 0.063034\n",
      "Loss at step 253000: 0.054250\n",
      "Loss at step 254000: 0.117240\n",
      "Loss at step 255000: 0.100979\n",
      "Validation at step 255000 MSE: 0.1569\n",
      "Loss at step 256000: 0.064956\n",
      "Loss at step 257000: 0.122550\n",
      "Loss at step 258000: 0.069177\n",
      "Loss at step 259000: 0.117580\n",
      "Loss at step 260000: 0.106197\n",
      "Validation at step 260000 MSE: 0.1565\n",
      "Loss at step 261000: 0.053934\n",
      "Loss at step 262000: 0.112784\n",
      "Loss at step 263000: 0.057680\n",
      "Loss at step 264000: 0.070051\n",
      "Loss at step 265000: 0.053788\n",
      "Validation at step 265000 MSE: 0.1557\n",
      "Loss at step 266000: 0.096765\n",
      "Loss at step 267000: 0.093691\n",
      "Loss at step 268000: 0.081122\n",
      "Loss at step 269000: 0.102031\n",
      "Loss at step 270000: 0.065708\n",
      "Validation at step 270000 MSE: 0.1562\n",
      "Loss at step 271000: 0.153371\n",
      "Loss at step 272000: 0.082531\n",
      "Loss at step 273000: 0.053503\n",
      "Loss at step 274000: 0.099851\n",
      "Loss at step 275000: 0.052783\n",
      "Validation at step 275000 MSE: 0.1552\n",
      "Loss at step 276000: 0.068436\n",
      "Loss at step 277000: 0.038956\n",
      "Loss at step 278000: 0.098148\n",
      "Loss at step 279000: 0.086167\n",
      "Loss at step 280000: 0.075379\n",
      "Validation at step 280000 MSE: 0.1548\n",
      "Loss at step 281000: 0.098024\n",
      "Loss at step 282000: 0.071826\n",
      "Loss at step 283000: 0.143908\n",
      "Loss at step 284000: 0.080737\n",
      "Loss at step 285000: 0.056708\n",
      "Validation at step 285000 MSE: 0.1553\n",
      "Loss at step 286000: 0.093239\n",
      "Loss at step 287000: 0.074778\n",
      "Loss at step 288000: 0.065481\n",
      "Loss at step 289000: 0.054032\n",
      "Loss at step 290000: 0.097846\n",
      "Validation at step 290000 MSE: 0.1548\n",
      "Loss at step 291000: 0.084826\n",
      "Loss at step 292000: 0.075769\n",
      "Loss at step 293000: 0.099461\n",
      "Loss at step 294000: 0.080569\n",
      "Loss at step 295000: 0.155947\n",
      "Validation at step 295000 MSE: 0.1554\n",
      "Loss at step 296000: 0.070867\n",
      "Loss at step 297000: 0.058746\n",
      "Loss at step 298000: 0.092877\n",
      "Loss at step 299000: 0.074148\n",
      "Loss at step 300000: 0.066956\n",
      "Validation at step 300000 MSE: 0.1547\n",
      "Test MSE: 0.1001\n",
      "[[ 1.         0.9253181]\n",
      " [ 0.9253181  1.       ]]\n",
      "[[ 1.          0.95920094]\n",
      " [ 0.95920094  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_output.shape[0] - batch_size)\n",
    "        batch_data = train_dataset2[offset:(offset + batch_size), :]\n",
    "        batch_output = train_output[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_output}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction],feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "        if (step % 5000 == 0):\n",
    "            # print('Training MSE: %.4f' % accuracy_rmse(predictions, train_output))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation at step %d MSE: %.4f' % (step,accuracy_mse(valid_prediction.eval(), valid_output)))\n",
    "    print('Test MSE: %.4f' % accuracy_mse(test_prediction.eval(), test_output))\n",
    "    predicted_vs_actual = np.hstack((test_prediction.eval(), test_output))\n",
    "\n",
    "print(np.corrcoef(predicted_vs_actual[:,0],predicted_vs_actual[:,2]))\n",
    "print(np.corrcoef(predicted_vs_actual[:,1],predicted_vs_actual[:,3]))\n",
    "\n",
    "#plt.plot(predicted_vs_actual[:1000,0],label=\"predicted\")\n",
    "#plt.plot(predicted_vs_actual[:1000,1],label=\"actual\")\n",
    "#plt.ylabel(\"normalized output\")\n",
    "#plt.legend(loc='upper right', shadow=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": True
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "stop = 193*10\n",
    "\n",
    "plt.plot(predicted_vs_actual[start:stop,0],label=\"predicted 1\")\n",
    "plt.plot(predicted_vs_actual[start:stop,2],label=\"actual 1\")\n",
    "plt.ylabel(\"normalized output\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(predicted_vs_actual[start:stop,1],label=\"predicted 2\")\n",
    "plt.plot(predicted_vs_actual[start:stop,3],label=\"actual 2\")\n",
    "plt.ylabel(\"normalized output\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": False
   },
   "outputs": [],
   "source": [
    "#print(graph)\n",
    "print(weights_1.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": True
   },
   "outputs": [],
   "source": [
    "# Deep ANN\n",
    "batch_size = 5*193\n",
    "hidden_nodes_1 = 25\n",
    "hidden_nodes_2 = 10\n",
    "hidden_nodes_3 = 10\n",
    "\n",
    "num_steps = 500001\n",
    "starter_learning_rate = 0.03\n",
    "#starter_learning_rate = 0.005\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 6)) #train_dataset2.shape(2)\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 2))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset2)\n",
    "    tf_test_dataset = tf.constant(test_dataset2)\n",
    "  \n",
    "    weights_0 = tf.Variable(tf.truncated_normal([6,hidden_nodes_1], dtype = tf.float32))\n",
    "    biases_0 = tf.Variable(tf.zeros([hidden_nodes_1], dtype = tf.float32))\n",
    "    \n",
    "    weights_1 = tf.Variable(tf.truncated_normal([hidden_nodes_1,hidden_nodes_2], dtype = tf.float32))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes_2], dtype = tf.float32))\n",
    "    \n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_2,hidden_nodes_3], dtype = tf.float32))\n",
    "    biases_2 = tf.Variable(tf.zeros([hidden_nodes_3], dtype = tf.float32))\n",
    "    \n",
    "    weights_3 = tf.Variable(tf.truncated_normal([hidden_nodes_3,2], dtype = tf.float32))\n",
    "    biases_3 = tf.Variable(tf.zeros([2], dtype = tf.float32))\n",
    "\n",
    "  \n",
    "    input_layer_output = tf.sigmoid(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "    hidden_layer_output = tf.sigmoid(tf.matmul(input_layer_output, weights_1) + biases_1)\n",
    "    #hidden_layer_output = tf.nn.dropout(hidden_layer_output, 0.5)\n",
    "    hidden_layer_output = tf.sigmoid(tf.matmul(hidden_layer_output, weights_2) + biases_2)\n",
    "    hidden_layer_output = tf.matmul(hidden_layer_output, weights_3) + biases_3\n",
    "    \n",
    "    \n",
    "    loss = tf.cast(tf.reduce_mean(tf.reduce_mean(tf.square(hidden_layer_output-tf_train_labels))),tf.float32)\n",
    "    #loss = tf.cast(tf.reduce_mean(tf.reduce_mean(tf.square(tf.square(hidden_layer_output-tf_train_labels)))),tf.float32)\n",
    "        \n",
    "    global_step = tf.Variable(0.00, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, num_steps, 0.96, staircase=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #starter_learning_rate = 0.5\n",
    "    #optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "    train_prediction = loss\n",
    "    valid_prediction = tf.sigmoid(tf.matmul(tf_valid_dataset, weights_0) + biases_0)\n",
    "    valid_prediction = tf.sigmoid(tf.matmul(valid_prediction, weights_1) + biases_1)\n",
    "    valid_prediction = tf.sigmoid(tf.matmul(valid_prediction, weights_2) + biases_2)\n",
    "    valid_prediction = tf.matmul(valid_prediction, weights_3) + biases_3\n",
    "    \n",
    "    test_prediction = tf.sigmoid(tf.matmul(tf_test_dataset, weights_0) + biases_0)\n",
    "    test_prediction = tf.sigmoid(tf.matmul(test_prediction, weights_1) + biases_1)\n",
    "    test_prediction = tf.sigmoid(tf.matmul(test_prediction, weights_2) + biases_2)\n",
    "    test_prediction = tf.matmul(test_prediction, weights_3) + biases_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": False
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 3.086511\n",
      "Validation at step 0 MSE: 2.4952\n",
      "Loss at step 1000: 0.415722\n",
      "Loss at step 2000: 0.242893\n",
      "Loss at step 3000: 0.657969\n",
      "Loss at step 4000: 0.859371\n",
      "Loss at step 5000: 0.225254\n",
      "Validation at step 5000 MSE: 0.4808\n",
      "Loss at step 6000: 0.441886\n",
      "Loss at step 7000: 0.258126\n",
      "Loss at step 8000: 0.624302\n",
      "Loss at step 9000: 0.114667\n",
      "Loss at step 10000: 0.376732\n",
      "Validation at step 10000 MSE: 0.3322\n",
      "Loss at step 11000: 0.276789\n",
      "Loss at step 12000: 0.231824\n",
      "Loss at step 13000: 0.308584\n",
      "Loss at step 14000: 0.307664\n",
      "Loss at step 15000: 0.297884\n",
      "Validation at step 15000 MSE: 0.2633\n",
      "Loss at step 16000: 0.130868\n",
      "Loss at step 17000: 0.273845\n",
      "Loss at step 18000: 0.106293\n",
      "Loss at step 19000: 0.299107\n",
      "Loss at step 20000: 0.407164\n",
      "Validation at step 20000 MSE: 0.2455\n",
      "Loss at step 21000: 0.159191\n",
      "Loss at step 22000: 0.195541\n",
      "Loss at step 23000: 0.086194\n",
      "Loss at step 24000: 0.135844\n",
      "Loss at step 25000: 0.140578\n",
      "Validation at step 25000 MSE: 0.2250\n",
      "Loss at step 26000: 0.222917\n",
      "Loss at step 27000: 0.128543\n",
      "Loss at step 28000: 0.094671\n",
      "Loss at step 29000: 0.201076\n",
      "Loss at step 30000: 0.153985\n",
      "Validation at step 30000 MSE: 0.2166\n",
      "Loss at step 31000: 0.118225\n",
      "Loss at step 32000: 0.286093\n",
      "Loss at step 33000: 0.067588\n",
      "Loss at step 34000: 0.223535\n",
      "Loss at step 35000: 0.197327\n",
      "Validation at step 35000 MSE: 0.2151\n",
      "Loss at step 36000: 0.126498\n",
      "Loss at step 37000: 0.225878\n",
      "Loss at step 38000: 0.139987\n",
      "Loss at step 39000: 0.130549\n",
      "Loss at step 40000: 0.055251\n",
      "Validation at step 40000 MSE: 0.2099\n",
      "Loss at step 41000: 0.084717\n",
      "Loss at step 42000: 0.112088\n",
      "Loss at step 43000: 0.269501\n",
      "Loss at step 44000: 0.075770\n",
      "Loss at step 45000: 0.111979\n",
      "Validation at step 45000 MSE: 0.2080\n",
      "Loss at step 46000: 0.086643\n",
      "Loss at step 47000: 0.093732\n",
      "Loss at step 48000: 0.155422\n",
      "Loss at step 49000: 0.076156\n",
      "Loss at step 50000: 0.076228\n",
      "Validation at step 50000 MSE: 0.1994\n",
      "Loss at step 51000: 0.091046\n",
      "Loss at step 52000: 0.219433\n",
      "Loss at step 53000: 0.102352\n",
      "Loss at step 54000: 0.138891\n",
      "Loss at step 55000: 0.079609\n",
      "Validation at step 55000 MSE: 0.1964\n",
      "Loss at step 56000: 0.108310\n",
      "Loss at step 57000: 0.087425\n",
      "Loss at step 58000: 0.062299\n",
      "Loss at step 59000: 0.047355\n",
      "Loss at step 60000: 0.118144\n",
      "Validation at step 60000 MSE: 0.1994\n",
      "Loss at step 61000: 0.148200\n",
      "Loss at step 62000: 0.079783\n",
      "Loss at step 63000: 0.205994\n",
      "Loss at step 64000: 0.180035\n",
      "Loss at step 65000: 0.057573\n",
      "Validation at step 65000 MSE: 0.1933\n",
      "Loss at step 66000: 0.135563\n",
      "Loss at step 67000: 0.138771\n",
      "Loss at step 68000: 0.049118\n",
      "Loss at step 69000: 0.073060\n",
      "Loss at step 70000: 0.161257\n",
      "Validation at step 70000 MSE: 0.1881\n",
      "Loss at step 71000: 0.081007\n",
      "Loss at step 72000: 0.059162\n",
      "Loss at step 73000: 0.058595\n",
      "Loss at step 74000: 0.087301\n",
      "Loss at step 75000: 0.093635\n",
      "Validation at step 75000 MSE: 0.1845\n",
      "Loss at step 76000: 0.215585\n",
      "Loss at step 77000: 0.078963\n",
      "Loss at step 78000: 0.156182\n",
      "Loss at step 79000: 0.140244\n",
      "Loss at step 80000: 0.087198\n",
      "Validation at step 80000 MSE: 0.1815\n",
      "Loss at step 81000: 0.181720\n",
      "Loss at step 82000: 0.126468\n",
      "Loss at step 83000: 0.069378\n",
      "Loss at step 84000: 0.056325\n",
      "Loss at step 85000: 0.077517\n",
      "Validation at step 85000 MSE: 0.1823\n",
      "Loss at step 86000: 0.100804\n",
      "Loss at step 87000: 0.201625\n",
      "Loss at step 88000: 0.088202\n",
      "Loss at step 89000: 0.103492\n",
      "Loss at step 90000: 0.106269\n",
      "Validation at step 90000 MSE: 0.1778\n",
      "Loss at step 91000: 0.100696\n",
      "Loss at step 92000: 0.160283\n",
      "Loss at step 93000: 0.143221\n",
      "Loss at step 94000: 0.047429\n",
      "Loss at step 95000: 0.077695\n",
      "Validation at step 95000 MSE: 0.1758\n",
      "Loss at step 96000: 0.128446\n",
      "Loss at step 97000: 0.094073\n",
      "Loss at step 98000: 0.109363\n",
      "Loss at step 99000: 0.049157\n",
      "Loss at step 100000: 0.052856\n",
      "Validation at step 100000 MSE: 0.1734\n",
      "Loss at step 101000: 0.146361\n",
      "Loss at step 102000: 0.106963\n",
      "Loss at step 103000: 0.057633\n",
      "Loss at step 104000: 0.136498\n",
      "Loss at step 105000: 0.066361\n",
      "Validation at step 105000 MSE: 0.1740\n",
      "Loss at step 106000: 0.074616\n",
      "Loss at step 107000: 0.059549\n",
      "Loss at step 108000: 0.121851\n",
      "Loss at step 109000: 0.033963\n",
      "Loss at step 110000: 0.135992\n",
      "Validation at step 110000 MSE: 0.1744\n",
      "Loss at step 111000: 0.192570\n",
      "Loss at step 112000: 0.103300\n",
      "Loss at step 113000: 0.060510\n",
      "Loss at step 114000: 0.064932\n",
      "Loss at step 115000: 0.149637\n",
      "Validation at step 115000 MSE: 0.1689\n",
      "Loss at step 116000: 0.059626\n",
      "Loss at step 117000: 0.078332\n",
      "Loss at step 118000: 0.042156\n",
      "Loss at step 119000: 0.081247\n",
      "Loss at step 120000: 0.054021\n",
      "Validation at step 120000 MSE: 0.1688\n",
      "Loss at step 121000: 0.114406\n",
      "Loss at step 122000: 0.084920\n",
      "Loss at step 123000: 0.080050\n",
      "Loss at step 124000: 0.048087\n",
      "Loss at step 125000: 0.121920\n",
      "Validation at step 125000 MSE: 0.1699\n",
      "Loss at step 126000: 0.096395\n",
      "Loss at step 127000: 0.055321\n",
      "Loss at step 128000: 0.112250\n",
      "Loss at step 129000: 0.070149\n",
      "Loss at step 130000: 0.199937\n",
      "Validation at step 130000 MSE: 0.1685\n",
      "Loss at step 131000: 0.156610\n",
      "Loss at step 132000: 0.061123\n",
      "Loss at step 133000: 0.081941\n",
      "Loss at step 134000: 0.068984\n",
      "Loss at step 135000: 0.079897\n",
      "Validation at step 135000 MSE: 0.1667\n",
      "Loss at step 136000: 0.076156\n",
      "Loss at step 137000: 0.138781\n",
      "Loss at step 138000: 0.067384\n",
      "Loss at step 139000: 0.057797\n",
      "Loss at step 140000: 0.106391\n",
      "Validation at step 140000 MSE: 0.1648\n",
      "Loss at step 141000: 0.080371\n",
      "Loss at step 142000: 0.074528\n",
      "Loss at step 143000: 0.157391\n",
      "Loss at step 144000: 0.039210\n",
      "Loss at step 145000: 0.117752\n",
      "Validation at step 145000 MSE: 0.1659\n",
      "Loss at step 146000: 0.101597\n",
      "Loss at step 147000: 0.060386\n",
      "Loss at step 148000: 0.140428\n",
      "Loss at step 149000: 0.102269\n",
      "Loss at step 150000: 0.093830\n",
      "Validation at step 150000 MSE: 0.1640\n",
      "Loss at step 151000: 0.028692\n",
      "Loss at step 152000: 0.054684\n",
      "Loss at step 153000: 0.065646\n",
      "Loss at step 154000: 0.216209\n",
      "Loss at step 155000: 0.067356\n",
      "Validation at step 155000 MSE: 0.1627\n",
      "Loss at step 156000: 0.105417\n",
      "Loss at step 157000: 0.086769\n",
      "Loss at step 158000: 0.072065\n",
      "Loss at step 159000: 0.112626\n",
      "Loss at step 160000: 0.060557\n",
      "Validation at step 160000 MSE: 0.1642\n",
      "Loss at step 161000: 0.050018\n",
      "Loss at step 162000: 0.053853\n",
      "Loss at step 163000: 0.139807\n",
      "Loss at step 164000: 0.064419\n",
      "Loss at step 165000: 0.091028\n",
      "Validation at step 165000 MSE: 0.1615\n",
      "Loss at step 166000: 0.055423\n",
      "Loss at step 167000: 0.078706\n",
      "Loss at step 168000: 0.062215\n",
      "Loss at step 169000: 0.046268\n",
      "Loss at step 170000: 0.027299\n",
      "Validation at step 170000 MSE: 0.1620\n",
      "Loss at step 171000: 0.082429\n",
      "Loss at step 172000: 0.092290\n",
      "Loss at step 173000: 0.068012\n",
      "Loss at step 174000: 0.155889\n",
      "Loss at step 175000: 0.119398\n",
      "Validation at step 175000 MSE: 0.1645\n",
      "Loss at step 176000: 0.041255\n",
      "Loss at step 177000: 0.103431\n",
      "Loss at step 178000: 0.104339\n",
      "Loss at step 179000: 0.032582\n",
      "Loss at step 180000: 0.069153\n",
      "Validation at step 180000 MSE: 0.1589\n",
      "Loss at step 181000: 0.114447\n",
      "Loss at step 182000: 0.069700\n",
      "Loss at step 183000: 0.040721\n",
      "Loss at step 184000: 0.049693\n",
      "Loss at step 185000: 0.059668\n",
      "Validation at step 185000 MSE: 0.1604\n",
      "Loss at step 186000: 0.074024\n",
      "Loss at step 187000: 0.152216\n",
      "Loss at step 188000: 0.066455\n",
      "Loss at step 189000: 0.123079\n",
      "Loss at step 190000: 0.107867\n",
      "Validation at step 190000 MSE: 0.1595\n",
      "Loss at step 191000: 0.069797\n",
      "Loss at step 192000: 0.150119\n",
      "Loss at step 193000: 0.110224\n",
      "Loss at step 194000: 0.054338\n",
      "Loss at step 195000: 0.044075\n",
      "Validation at step 195000 MSE: 0.1626\n",
      "Loss at step 196000: 0.060621\n",
      "Loss at step 197000: 0.079513\n",
      "Loss at step 198000: 0.181351\n",
      "Loss at step 199000: 0.074726\n",
      "Loss at step 200000: 0.078155\n",
      "Validation at step 200000 MSE: 0.1607\n",
      "Loss at step 201000: 0.093809\n",
      "Loss at step 202000: 0.095847\n",
      "Loss at step 203000: 0.145052\n",
      "Loss at step 204000: 0.120292\n",
      "Loss at step 205000: 0.044020\n",
      "Validation at step 205000 MSE: 0.1564\n",
      "Loss at step 206000: 0.063561\n",
      "Loss at step 207000: 0.106575\n",
      "Loss at step 208000: 0.080179\n",
      "Loss at step 209000: 0.078698\n",
      "Loss at step 210000: 0.037328\n",
      "Validation at step 210000 MSE: 0.1625\n",
      "Loss at step 211000: 0.050950\n",
      "Loss at step 212000: 0.125656\n",
      "Loss at step 213000: 0.072177\n",
      "Loss at step 214000: 0.045425\n",
      "Loss at step 215000: 0.117524\n",
      "Validation at step 215000 MSE: 0.1593\n",
      "Loss at step 216000: 0.052181\n",
      "Loss at step 217000: 0.068882\n",
      "Loss at step 218000: 0.045191\n",
      "Loss at step 219000: 0.090924\n",
      "Loss at step 220000: 0.026794\n",
      "Validation at step 220000 MSE: 0.1604\n",
      "Loss at step 221000: 0.126670\n",
      "Loss at step 222000: 0.148364\n",
      "Loss at step 223000: 0.085524\n",
      "Loss at step 224000: 0.055872\n",
      "Loss at step 225000: 0.052472\n",
      "Validation at step 225000 MSE: 0.1581\n",
      "Loss at step 226000: 0.143356\n",
      "Loss at step 227000: 0.053593\n",
      "Loss at step 228000: 0.068401\n",
      "Loss at step 229000: 0.029594\n",
      "Loss at step 230000: 0.062093\n",
      "Validation at step 230000 MSE: 0.1577\n",
      "Loss at step 231000: 0.045267\n",
      "Loss at step 232000: 0.096141\n",
      "Loss at step 233000: 0.071225\n",
      "Loss at step 234000: 0.071857\n",
      "Loss at step 235000: 0.039634\n",
      "Validation at step 235000 MSE: 0.1601\n",
      "Loss at step 236000: 0.110192\n",
      "Loss at step 237000: 0.083608\n",
      "Loss at step 238000: 0.043986\n",
      "Loss at step 239000: 0.085870\n",
      "Loss at step 240000: 0.062325\n",
      "Validation at step 240000 MSE: 0.1586\n",
      "Loss at step 241000: 0.164217\n",
      "Loss at step 242000: 0.145226\n",
      "Loss at step 243000: 0.055645\n",
      "Loss at step 244000: 0.064658\n",
      "Loss at step 245000: 0.059930\n",
      "Validation at step 245000 MSE: 0.1579\n",
      "Loss at step 246000: 0.074871\n",
      "Loss at step 247000: 0.064509\n",
      "Loss at step 248000: 0.124286\n",
      "Loss at step 249000: 0.060855\n",
      "Loss at step 250000: 0.049059\n",
      "Validation at step 250000 MSE: 0.1578\n",
      "Loss at step 251000: 0.093708\n",
      "Loss at step 252000: 0.074658\n",
      "Loss at step 253000: 0.062544\n",
      "Loss at step 254000: 0.127934\n",
      "Loss at step 255000: 0.038541\n",
      "Validation at step 255000 MSE: 0.1561\n",
      "Loss at step 256000: 0.099398\n",
      "Loss at step 257000: 0.086201\n",
      "Loss at step 258000: 0.055059\n",
      "Loss at step 259000: 0.120849\n",
      "Loss at step 260000: 0.096181\n",
      "Validation at step 260000 MSE: 0.1599\n",
      "Loss at step 261000: 0.082287\n",
      "Loss at step 262000: 0.026980\n",
      "Loss at step 263000: 0.046004\n",
      "Loss at step 264000: 0.059094\n",
      "Loss at step 265000: 0.201097\n",
      "Validation at step 265000 MSE: 0.1576\n",
      "Loss at step 266000: 0.061831\n",
      "Loss at step 267000: 0.091836\n",
      "Loss at step 268000: 0.078444\n",
      "Loss at step 269000: 0.064478\n",
      "Loss at step 270000: 0.107699\n",
      "Validation at step 270000 MSE: 0.1568\n",
      "Loss at step 271000: 0.059149\n",
      "Loss at step 272000: 0.038464\n",
      "Loss at step 273000: 0.047282\n",
      "Loss at step 274000: 0.118095\n",
      "Loss at step 275000: 0.053658\n",
      "Validation at step 275000 MSE: 0.1579\n",
      "Loss at step 276000: 0.075982\n",
      "Loss at step 277000: 0.048524\n",
      "Loss at step 278000: 0.073927\n",
      "Loss at step 279000: 0.054697\n",
      "Loss at step 280000: 0.040423\n",
      "Validation at step 280000 MSE: 0.1548\n",
      "Loss at step 281000: 0.024188\n",
      "Loss at step 282000: 0.071769\n",
      "Loss at step 283000: 0.076050\n",
      "Loss at step 284000: 0.066423\n",
      "Loss at step 285000: 0.133984\n",
      "Validation at step 285000 MSE: 0.1566\n",
      "Loss at step 286000: 0.092447\n",
      "Loss at step 287000: 0.038485\n",
      "Loss at step 288000: 0.095258\n",
      "Loss at step 289000: 0.087800\n",
      "Loss at step 290000: 0.031005\n",
      "Validation at step 290000 MSE: 0.1550\n",
      "Loss at step 291000: 0.056277\n",
      "Loss at step 292000: 0.097776\n",
      "Loss at step 293000: 0.066867\n",
      "Loss at step 294000: 0.035529\n",
      "Loss at step 295000: 0.048291\n",
      "Validation at step 295000 MSE: 0.1515\n",
      "Loss at step 296000: 0.055109\n",
      "Loss at step 297000: 0.062188\n",
      "Loss at step 298000: 0.126099\n",
      "Loss at step 299000: 0.067139\n",
      "Loss at step 300000: 0.105425\n",
      "Validation at step 300000 MSE: 0.1602\n",
      "Loss at step 301000: 0.101485\n",
      "Loss at step 302000: 0.063088\n",
      "Loss at step 303000: 0.133129\n",
      "Loss at step 304000: 0.103508\n",
      "Loss at step 305000: 0.045804\n",
      "Validation at step 305000 MSE: 0.1545\n",
      "Loss at step 306000: 0.040461\n",
      "Loss at step 307000: 0.056585\n",
      "Loss at step 308000: 0.069725\n",
      "Loss at step 309000: 0.174367\n",
      "Loss at step 310000: 0.068455\n",
      "Validation at step 310000 MSE: 0.1553\n",
      "Loss at step 311000: 0.072111\n",
      "Loss at step 312000: 0.086690\n",
      "Loss at step 313000: 0.097378\n",
      "Loss at step 314000: 0.131600\n",
      "Loss at step 315000: 0.108800\n",
      "Validation at step 315000 MSE: 0.1574\n",
      "Loss at step 316000: 0.037883\n",
      "Loss at step 317000: 0.059235\n",
      "Loss at step 318000: 0.094759\n",
      "Loss at step 319000: 0.077816\n",
      "Loss at step 320000: 0.061485\n",
      "Validation at step 320000 MSE: 0.1544\n",
      "Loss at step 321000: 0.034422\n",
      "Loss at step 322000: 0.051525\n",
      "Loss at step 323000: 0.113764\n",
      "Loss at step 324000: 0.062505\n",
      "Loss at step 325000: 0.042577\n",
      "Validation at step 325000 MSE: 0.1563\n",
      "Loss at step 326000: 0.106025\n",
      "Loss at step 327000: 0.047435\n",
      "Loss at step 328000: 0.064454\n",
      "Loss at step 329000: 0.038160\n",
      "Loss at step 330000: 0.068055\n",
      "Validation at step 330000 MSE: 0.1562\n",
      "Loss at step 331000: 0.023229\n",
      "Loss at step 332000: 0.119769\n",
      "Loss at step 333000: 0.120320\n",
      "Loss at step 334000: 0.075101\n",
      "Loss at step 335000: 0.043941\n",
      "Validation at step 335000 MSE: 0.1517\n",
      "Loss at step 336000: 0.047381\n",
      "Loss at step 337000: 0.136984\n",
      "Loss at step 338000: 0.049927\n",
      "Loss at step 339000: 0.063679\n",
      "Loss at step 340000: 0.024178\n",
      "Validation at step 340000 MSE: 0.1557\n",
      "Loss at step 341000: 0.053519\n",
      "Loss at step 342000: 0.039498\n",
      "Loss at step 343000: 0.084052\n",
      "Loss at step 344000: 0.062137\n",
      "Loss at step 345000: 0.068487\n",
      "Validation at step 345000 MSE: 0.1537\n",
      "Loss at step 346000: 0.033781\n",
      "Loss at step 347000: 0.104500\n",
      "Loss at step 348000: 0.079709\n",
      "Loss at step 349000: 0.035043\n",
      "Loss at step 350000: 0.079310\n",
      "Validation at step 350000 MSE: 0.1614\n",
      "Loss at step 351000: 0.061028\n",
      "Loss at step 352000: 0.143422\n",
      "Loss at step 353000: 0.137777\n",
      "Loss at step 354000: 0.052287\n",
      "Loss at step 355000: 0.061009\n",
      "Validation at step 355000 MSE: 0.1539\n",
      "Loss at step 356000: 0.056405\n",
      "Loss at step 357000: 0.071932\n",
      "Loss at step 358000: 0.055525\n",
      "Loss at step 359000: 0.109597\n",
      "Loss at step 360000: 0.053195\n",
      "Validation at step 360000 MSE: 0.1512\n",
      "Loss at step 361000: 0.046913\n",
      "Loss at step 362000: 0.087418\n",
      "Loss at step 363000: 0.073870\n",
      "Loss at step 364000: 0.056589\n",
      "Loss at step 365000: 0.109502\n",
      "Validation at step 365000 MSE: 0.1628\n",
      "Loss at step 366000: 0.038566\n",
      "Loss at step 367000: 0.085443\n",
      "Loss at step 368000: 0.081692\n",
      "Loss at step 369000: 0.051424\n",
      "Loss at step 370000: 0.106826\n",
      "Validation at step 370000 MSE: 0.1545\n",
      "Loss at step 371000: 0.090697\n",
      "Loss at step 372000: 0.074996\n",
      "Loss at step 373000: 0.025984\n",
      "Loss at step 374000: 0.042916\n",
      "Loss at step 375000: 0.056971\n",
      "Validation at step 375000 MSE: 0.1545\n",
      "Loss at step 376000: 0.187058\n",
      "Loss at step 377000: 0.055949\n",
      "Loss at step 378000: 0.083147\n",
      "Loss at step 379000: 0.071830\n",
      "Loss at step 380000: 0.059591\n",
      "Validation at step 380000 MSE: 0.1555\n",
      "Loss at step 381000: 0.101704\n",
      "Loss at step 382000: 0.051658\n",
      "Loss at step 383000: 0.032837\n",
      "Loss at step 384000: 0.044892\n",
      "Loss at step 385000: 0.112438\n",
      "Validation at step 385000 MSE: 0.1562\n",
      "Loss at step 386000: 0.049290\n",
      "Loss at step 387000: 0.066812\n",
      "Loss at step 388000: 0.043587\n",
      "Loss at step 389000: 0.074827\n",
      "Loss at step 390000: 0.051048\n",
      "Validation at step 390000 MSE: 0.1598\n",
      "Loss at step 391000: 0.038867\n",
      "Loss at step 392000: 0.023920\n",
      "Loss at step 393000: 0.064054\n",
      "Loss at step 394000: 0.075135\n",
      "Loss at step 395000: 0.064046\n",
      "Validation at step 395000 MSE: 0.1555\n",
      "Loss at step 396000: 0.119442\n",
      "Loss at step 397000: 0.073937\n",
      "Loss at step 398000: 0.036772\n",
      "Loss at step 399000: 0.091024\n",
      "Loss at step 400000: 0.081818\n",
      "Validation at step 400000 MSE: 0.1546\n",
      "Loss at step 401000: 0.026331\n",
      "Loss at step 402000: 0.049113\n",
      "Loss at step 403000: 0.086776\n",
      "Loss at step 404000: 0.068210\n",
      "Loss at step 405000: 0.030911\n",
      "Validation at step 405000 MSE: 0.1563\n",
      "Loss at step 406000: 0.049020\n",
      "Loss at step 407000: 0.054136\n",
      "Loss at step 408000: 0.056702\n",
      "Loss at step 409000: 0.108781\n",
      "Loss at step 410000: 0.067482\n",
      "Validation at step 410000 MSE: 0.1546\n",
      "Loss at step 411000: 0.090662\n",
      "Loss at step 412000: 0.102301\n",
      "Loss at step 413000: 0.057827\n",
      "Loss at step 414000: 0.126699\n",
      "Loss at step 415000: 0.099014\n",
      "Validation at step 415000 MSE: 0.1620\n",
      "Loss at step 416000: 0.042069\n",
      "Loss at step 417000: 0.037878\n",
      "Loss at step 418000: 0.055330\n",
      "Loss at step 419000: 0.064933\n",
      "Loss at step 420000: 0.165766\n",
      "Validation at step 420000 MSE: 0.1537\n",
      "Loss at step 421000: 0.063119\n",
      "Loss at step 422000: 0.067902\n",
      "Loss at step 423000: 0.081210\n",
      "Loss at step 424000: 0.094296\n",
      "Loss at step 425000: 0.118356\n",
      "Validation at step 425000 MSE: 0.1573\n",
      "Loss at step 426000: 0.100774\n",
      "Loss at step 427000: 0.033771\n",
      "Loss at step 428000: 0.058306\n",
      "Loss at step 429000: 0.091059\n",
      "Loss at step 430000: 0.079072\n",
      "Validation at step 430000 MSE: 0.1590\n",
      "Loss at step 431000: 0.053621\n",
      "Loss at step 432000: 0.032669\n",
      "Loss at step 433000: 0.054578\n",
      "Loss at step 434000: 0.103831\n",
      "Loss at step 435000: 0.059056\n",
      "Validation at step 435000 MSE: 0.1529\n",
      "Loss at step 436000: 0.041180\n",
      "Loss at step 437000: 0.104050\n",
      "Loss at step 438000: 0.047344\n",
      "Loss at step 439000: 0.060349\n",
      "Loss at step 440000: 0.033762\n",
      "Validation at step 440000 MSE: 0.1604\n",
      "Loss at step 441000: 0.056270\n",
      "Loss at step 442000: 0.022717\n",
      "Loss at step 443000: 0.113383\n",
      "Loss at step 444000: 0.108564\n",
      "Loss at step 445000: 0.067169\n",
      "Validation at step 445000 MSE: 0.1549\n",
      "Loss at step 446000: 0.038611\n",
      "Loss at step 447000: 0.040901\n",
      "Loss at step 448000: 0.133592\n",
      "Loss at step 449000: 0.046329\n",
      "Loss at step 450000: 0.061425\n",
      "Validation at step 450000 MSE: 0.1502\n",
      "Loss at step 451000: 0.022238\n",
      "Loss at step 452000: 0.049671\n",
      "Loss at step 453000: 0.037212\n",
      "Loss at step 454000: 0.078393\n",
      "Loss at step 455000: 0.056754\n",
      "Validation at step 455000 MSE: 0.1623\n",
      "Loss at step 456000: 0.066889\n",
      "Loss at step 457000: 0.030342\n",
      "Loss at step 458000: 0.101287\n",
      "Loss at step 459000: 0.078744\n",
      "Loss at step 460000: 0.030932\n",
      "Validation at step 460000 MSE: 0.1564\n",
      "Loss at step 461000: 0.080816\n",
      "Loss at step 462000: 0.058791\n",
      "Loss at step 463000: 0.127552\n",
      "Loss at step 464000: 0.133594\n",
      "Loss at step 465000: 0.049837\n",
      "Validation at step 465000 MSE: 0.1559\n",
      "Loss at step 466000: 0.058536\n",
      "Loss at step 467000: 0.053643\n",
      "Loss at step 468000: 0.068279\n",
      "Loss at step 469000: 0.050635\n",
      "Loss at step 470000: 0.101417\n",
      "Validation at step 470000 MSE: 0.1583\n",
      "Loss at step 471000: 0.046704\n",
      "Loss at step 472000: 0.046328\n",
      "Loss at step 473000: 0.086375\n",
      "Loss at step 474000: 0.074298\n",
      "Loss at step 475000: 0.053767\n",
      "Validation at step 475000 MSE: 0.1569\n",
      "Loss at step 476000: 0.095961\n",
      "Loss at step 477000: 0.038783\n",
      "Loss at step 478000: 0.076968\n",
      "Loss at step 479000: 0.078687\n",
      "Loss at step 480000: 0.048754\n",
      "Validation at step 480000 MSE: 0.1588\n",
      "Loss at step 481000: 0.103042\n",
      "Loss at step 482000: 0.086260\n",
      "Loss at step 483000: 0.070037\n",
      "Loss at step 484000: 0.024691\n",
      "Loss at step 485000: 0.042364\n",
      "Validation at step 485000 MSE: 0.1571\n",
      "Loss at step 486000: 0.057708\n",
      "Loss at step 487000: 0.177438\n",
      "Loss at step 488000: 0.050708\n",
      "Loss at step 489000: 0.075676\n",
      "Loss at step 490000: 0.070245\n",
      "Validation at step 490000 MSE: 0.1532\n",
      "Loss at step 491000: 0.054719\n",
      "Loss at step 492000: 0.096564\n",
      "Loss at step 493000: 0.047425\n",
      "Loss at step 494000: 0.030800\n",
      "Loss at step 495000: 0.044741\n",
      "Validation at step 495000 MSE: 0.1578\n",
      "Loss at step 496000: 0.109100\n",
      "Loss at step 497000: 0.047283\n",
      "Loss at step 498000: 0.064567\n",
      "Loss at step 499000: 0.041711\n",
      "Loss at step 500000: 0.075999\n",
      "Validation at step 500000 MSE: 0.1562\n",
      "Test RMSE: 0.0848\n",
      "[[ 1.          0.93410434]\n",
      " [ 0.93410434  1.        ]]\n",
      "[[ 1.          0.96925351]\n",
      " [ 0.96925351  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_output.shape[0] - batch_size)\n",
    "        batch_data = train_dataset2[offset:(offset + batch_size), :]\n",
    "        batch_output = train_output[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_output}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction],feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "        if (step % 5000 == 0):\n",
    "            # print('Training MSE: %.4f' % accuracy_rmse(predictions, train_output))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation at step %d MSE: %.4f' % (step,accuracy_mse(valid_prediction.eval(), valid_output)))\n",
    "    print('Test RMSE: %.4f' % accuracy_mse(test_prediction.eval(), test_output))\n",
    "    predicted_vs_actual = np.hstack((test_prediction.eval(), test_output))\n",
    "\n",
    "print(np.corrcoef(predicted_vs_actual[:,0],predicted_vs_actual[:,2]))\n",
    "print(np.corrcoef(predicted_vs_actual[:,1],predicted_vs_actual[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": True
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "stop = 193*10\n",
    "\n",
    "plt.plot(predicted_vs_actual[start:stop,0],label=\"predicted 1\")\n",
    "plt.plot(predicted_vs_actual[start:stop,2],label=\"actual 1\")\n",
    "plt.ylabel(\"normalized output\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(predicted_vs_actual[start:stop,1],label=\"predicted 2\")\n",
    "plt.plot(predicted_vs_actual[start:stop,3],label=\"actual 2\")\n",
    "plt.ylabel(\"normalized output\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
