{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ann_dataset1.tar\n",
      "http://mrtee.europa.renci.org/~bblanton/ANN/ann_dataset1.tar\n",
      "Found and verified ann_dataset1.tar\n"
     ]
    }
   ],
   "source": [
    "# Download and save the archived data\n",
    "\n",
    "url = 'http://mrtee.europa.renci.org/~bblanton/ANN/'\n",
    "to = \"../data\"\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    print(os.path.join(to,filename))\n",
    "    print(url+filename)\n",
    "    if force or not os.path.exists(os.path.join(to,filename)):\n",
    "        filename, _ = urlretrieve(url + filename, os.path.join(to,filename))\n",
    "    statinfo = os.stat(os.path.join(to,filename))\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        raise Exception(\n",
    "          'Failed to verify' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "data_filename = maybe_download('ann_dataset1.tar', 5642240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data already present - Skipping extraction of ../data/ann_dataset1.tar.\n"
     ]
    }
   ],
   "source": [
    "# Extract files from the archive\n",
    "def maybe_extract(filename, force=False):\n",
    "    extract_folder = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    root = os.path.dirname(filename)\n",
    "    if os.path.isdir(extract_folder) and not force:\n",
    "    # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(path = root)\n",
    "        tar.close()\n",
    "    data_files = [\n",
    "        os.path.join(extract_folder, d) for d in sorted(os.listdir(extract_folder))\n",
    "        if os.path.isdir(extract_folder)]\n",
    "    return data_files\n",
    "  \n",
    "data_filename = \"../data/ann_dataset1.tar\"\n",
    "data_files = maybe_extract(data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/324 \n",
      "\n",
      "Processed 100/324 \n",
      "\n",
      "Processed 200/324 \n",
      "\n",
      "Processed 300/324 \n",
      "\n",
      "(324, 193, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load files and produce dataset\n",
    "def maybe_load(file_names):\n",
    "    names = ('index','time', 'long', 'lat', 'param1', 'param2', 'param3', 'param4', 'out1', 'out2')\n",
    "    datafile_length = 193\n",
    "    dataset = np.ndarray(shape=(len(file_names), datafile_length, len(names)))\n",
    "    for i in range(0,len(file_names)):\n",
    "        a = np.loadtxt(file_names[i])\n",
    "        a = np.asarray([x for xs in a for x in xs],dtype='d').reshape([datafile_length,len(names)])\n",
    "        dataset[i,:,:] = a\n",
    "        if i%100 == 0:\n",
    "            print(\"Processed %d/%d \\n\"%(i,len(file_names)))\n",
    "    return dataset\n",
    "\n",
    "dataset = maybe_load(data_files)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset: (48, 193, 10)\n",
      "Validation dataset: (49, 193, 10)\n",
      "Training dataset: (227, 193, 10)\n"
     ]
    }
   ],
   "source": [
    "# train, validation, and test dataset percentages\n",
    "train_percent = 70\n",
    "valid_percent = 15\n",
    "test_percent = 15\n",
    "\n",
    "# train, validation, and test dataset indices\n",
    "# test: test_start : valid_start-1\n",
    "# validation: valid_start : train_start-1\n",
    "# training: train_start : dataset.shape[0]\n",
    "test_start = 0 \n",
    "valid_start = int(test_percent/100.0*dataset.shape[0])\n",
    "train_start = int((test_percent+valid_percent)/100.0*dataset.shape[0])\n",
    "\n",
    "# Shuffle file indices\n",
    "file_indices = range(dataset.shape[0])\n",
    "np.random.shuffle(file_indices)\n",
    "\n",
    "# Assign datasets\n",
    "test_dataset = np.array([dataset[j,:,:] for j in [file_indices[i] for i in range(test_start, valid_start)]])\n",
    "valid_dataset = np.array([dataset[j,:,:] for j in [file_indices[i] for i in range(valid_start, train_start)]])\n",
    "train_dataset = np.array([dataset[j,:,:] for j in [file_indices[i] for i in range(train_start, dataset.shape[0])]])\n",
    "\n",
    "# Save memory\n",
    "#del(dataset)\n",
    "print(\"Test dataset: \"+str(test_dataset.shape))\n",
    "print(\"Validation dataset: \"+str(valid_dataset.shape))\n",
    "print(\"Training dataset: \"+str(train_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy_mse(predictions, outputs):\n",
    "    err = predictions-outputs\n",
    "    return np.mean(err*err)\n",
    "\n",
    "def Normalize(x, means, stds):\n",
    "    return (x-means)/stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43811, 6)\n",
      "[[-0.26237872 -0.02687204]\n",
      " [-0.25926641 -0.03060128]\n",
      " [-0.25793257 -0.03731391]\n",
      " ..., \n",
      " [-2.46878481 -1.39774013]\n",
      " [-2.46878481 -1.39774013]\n",
      " [-2.46878481 -1.39774013]]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data and normalize\n",
    "\n",
    "train_dataset2 = train_dataset[:,:,1:7].reshape((-1, 6)).astype(np.float32)\n",
    "train_output = train_dataset[:,:,8:10].reshape((-1, 2)).astype(np.float32)\n",
    "\n",
    "# calculate means and stds for training dataset\n",
    "input_means = [np.mean(train_dataset2[:,i]) for i in range(train_dataset2.shape[1])]\n",
    "input_stds = [np.std(train_dataset2[:,i]) for i in range(train_dataset2.shape[1])]\n",
    "output_means = [np.mean(train_output[:,i]) for i in range(train_output.shape[1])]\n",
    "output_stds = [np.std(train_output[:,i]) for i in range(train_output.shape[1])]\n",
    "\n",
    "train_dataset2 = Normalize(train_dataset2, input_means, input_stds)\n",
    "train_output = Normalize(train_output, output_means, output_stds)\n",
    "\n",
    "print(train_dataset2.shape)\n",
    "print(train_output)\n",
    "\n",
    "plt.plot(train_dataset2[:193,:],label=\"input\")\n",
    "plt.plot(train_output[:193,:],label=\"output\")\n",
    "plt.ylabel(\"training data\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()\n",
    "\n",
    "valid_dataset2 = Normalize(valid_dataset[:,:,1:7].reshape((-1, 6)).astype(np.float32), input_means, input_stds)\n",
    "valid_output = Normalize(valid_dataset[:,:,8:10].reshape((-1, 2)).astype(np.float32), output_means, output_stds)\n",
    "\n",
    "test_dataset2 = Normalize(test_dataset[:,:,1:7].reshape((-1, 6)).astype(np.float32),input_means, input_stds)\n",
    "test_output = Normalize(test_dataset[:,:,8:10].reshape((-1, 2)).astype(np.float32), output_means, output_stds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DoubleGDOptimizer(tf.train.GradientDescentOptimizer):\n",
    "  def _valid_dtypes(self):\n",
    "    return set([tf.float32, tf.float64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deep ANN\n",
    "batch_size = 20*193\n",
    "hidden_nodes_1 = 30\n",
    "hidden_nodes_2 = 10\n",
    "hidden_nodes_3 = 6\n",
    "\n",
    "num_steps = 500001\n",
    "starter_learning_rate = 0.05\n",
    "#starter_learning_rate = 0.005\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 6)) #train_dataset2.shape(2)\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 2))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset2)\n",
    "    tf_test_dataset = tf.constant(test_dataset2)\n",
    "  \n",
    "    weights_0 = tf.Variable(tf.truncated_normal([6,hidden_nodes_1], dtype = tf.float32))\n",
    "    biases_0 = tf.Variable(tf.zeros([hidden_nodes_1], dtype = tf.float32))\n",
    "    \n",
    "    weights_1 = tf.Variable(tf.truncated_normal([hidden_nodes_1,hidden_nodes_2], dtype = tf.float32))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes_2], dtype = tf.float32))\n",
    "    \n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_2,2], dtype = tf.float32))\n",
    "    biases_2 = tf.Variable(tf.zeros([2], dtype = tf.float32))\n",
    "\n",
    "  \n",
    "    input_layer_output = tf.sigmoid(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "    hidden_layer_output = tf.sigmoid(tf.matmul(input_layer_output, weights_1) + biases_1)\n",
    "    #hidden_layer_output = tf.nn.dropout(hidden_layer_output, 0.5)\n",
    "    hidden_layer_output = tf.matmul(hidden_layer_output, weights_2) + biases_2\n",
    "    \n",
    "    \n",
    "    loss = tf.cast(tf.reduce_mean(tf.reduce_mean(tf.square(hidden_layer_output-tf_train_labels))),tf.float32)\n",
    "    #loss = tf.cast(tf.reduce_mean(tf.reduce_mean(tf.square(tf.square(hidden_layer_output-tf_train_labels)))),tf.float32)\n",
    "        \n",
    "    global_step = tf.Variable(0.00, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, num_steps, 0.96, staircase=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #starter_learning_rate = 0.5\n",
    "    #optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "    train_prediction = loss\n",
    "    valid_prediction = tf.sigmoid(tf.matmul(tf_valid_dataset, weights_0) + biases_0)\n",
    "    valid_prediction = tf.sigmoid(tf.matmul(valid_prediction, weights_1) + biases_1)\n",
    "    valid_prediction = tf.matmul(valid_prediction, weights_2) + biases_2\n",
    "    \n",
    "    test_prediction = tf.sigmoid(tf.matmul(tf_test_dataset, weights_0) + biases_0)\n",
    "    test_prediction = tf.sigmoid(tf.matmul(test_prediction, weights_1) + biases_1)\n",
    "    test_prediction = tf.matmul(test_prediction, weights_2) + biases_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 4.153788\n",
      "Validation at step 0 MSE: 3.1821\n",
      "Loss at step 1000: 0.250329\n",
      "Loss at step 2000: 0.620206\n",
      "Loss at step 3000: 0.538030\n",
      "Loss at step 4000: 0.386047\n",
      "Loss at step 5000: 0.547600\n",
      "Validation at step 5000 MSE: 0.3599\n",
      "Loss at step 6000: 0.250856\n",
      "Loss at step 7000: 0.363866\n",
      "Loss at step 8000: 0.197658\n",
      "Loss at step 9000: 0.220661\n",
      "Loss at step 10000: 0.268949\n",
      "Validation at step 10000 MSE: 0.2332\n",
      "Loss at step 11000: 0.317938\n",
      "Loss at step 12000: 0.305610\n",
      "Loss at step 13000: 0.157492\n",
      "Loss at step 14000: 0.101489\n",
      "Loss at step 15000: 0.216653\n",
      "Validation at step 15000 MSE: 0.1728\n",
      "Loss at step 16000: 0.207376\n",
      "Loss at step 17000: 0.358684\n",
      "Loss at step 18000: 0.174050\n",
      "Loss at step 19000: 0.197096\n",
      "Loss at step 20000: 0.136949\n",
      "Validation at step 20000 MSE: 0.1527\n",
      "Loss at step 21000: 0.139722\n",
      "Loss at step 22000: 0.156665\n",
      "Loss at step 23000: 0.180068\n",
      "Loss at step 24000: 0.255785\n",
      "Loss at step 25000: 0.127503\n",
      "Validation at step 25000 MSE: 0.1365\n",
      "Loss at step 26000: 0.089658\n",
      "Loss at step 27000: 0.134310\n",
      "Loss at step 28000: 0.177263\n",
      "Loss at step 29000: 0.309940\n",
      "Loss at step 30000: 0.138463\n",
      "Validation at step 30000 MSE: 0.1332\n",
      "Loss at step 31000: 0.137487\n",
      "Loss at step 32000: 0.119044\n",
      "Loss at step 33000: 0.122131\n",
      "Loss at step 34000: 0.149942\n",
      "Loss at step 35000: 0.140783\n",
      "Validation at step 35000 MSE: 0.1280\n",
      "Loss at step 36000: 0.240883\n",
      "Loss at step 37000: 0.125249\n",
      "Loss at step 38000: 0.088079\n",
      "Loss at step 39000: 0.114511\n",
      "Loss at step 40000: 0.159705\n",
      "Validation at step 40000 MSE: 0.1221\n",
      "Loss at step 41000: 0.250477\n",
      "Loss at step 42000: 0.136844\n",
      "Loss at step 43000: 0.079232\n",
      "Loss at step 44000: 0.111365\n",
      "Loss at step 45000: 0.108908\n",
      "Validation at step 45000 MSE: 0.1222\n",
      "Loss at step 46000: 0.133232\n",
      "Loss at step 47000: 0.130936\n",
      "Loss at step 48000: 0.165813\n",
      "Loss at step 49000: 0.116524\n",
      "Loss at step 50000: 0.076950\n",
      "Validation at step 50000 MSE: 0.1169\n",
      "Loss at step 51000: 0.100469\n",
      "Loss at step 52000: 0.147287\n",
      "Loss at step 53000: 0.241835\n",
      "Loss at step 54000: 0.117171\n",
      "Loss at step 55000: 0.076516\n",
      "Validation at step 55000 MSE: 0.1155\n",
      "Loss at step 56000: 0.100603\n",
      "Loss at step 57000: 0.114961\n",
      "Loss at step 58000: 0.117874\n",
      "Loss at step 59000: 0.110433\n",
      "Loss at step 60000: 0.149753\n",
      "Validation at step 60000 MSE: 0.1133\n",
      "Loss at step 61000: 0.094707\n",
      "Loss at step 62000: 0.072076\n",
      "Loss at step 63000: 0.088506\n",
      "Loss at step 64000: 0.128140\n",
      "Loss at step 65000: 0.237994\n",
      "Validation at step 65000 MSE: 0.1118\n",
      "Loss at step 66000: 0.107948\n",
      "Loss at step 67000: 0.057562\n",
      "Loss at step 68000: 0.091814\n",
      "Loss at step 69000: 0.115271\n",
      "Loss at step 70000: 0.100040\n",
      "Validation at step 70000 MSE: 0.1106\n",
      "Loss at step 71000: 0.096052\n",
      "Loss at step 72000: 0.157912\n",
      "Loss at step 73000: 0.104051\n",
      "Loss at step 74000: 0.059158\n",
      "Loss at step 75000: 0.095142\n",
      "Validation at step 75000 MSE: 0.1073\n",
      "Loss at step 76000: 0.122534\n",
      "Loss at step 77000: 0.269840\n",
      "Loss at step 78000: 0.107089\n",
      "Loss at step 79000: 0.053290\n",
      "Loss at step 80000: 0.080839\n",
      "Validation at step 80000 MSE: 0.1065\n",
      "Loss at step 81000: 0.110817\n",
      "Loss at step 82000: 0.093074\n",
      "Loss at step 83000: 0.093740\n",
      "Loss at step 84000: 0.153825\n",
      "Loss at step 85000: 0.096419\n",
      "Validation at step 85000 MSE: 0.1054\n",
      "Loss at step 86000: 0.131554\n",
      "Loss at step 87000: 0.086463\n",
      "Loss at step 88000: 0.123289\n",
      "Loss at step 89000: 0.242709\n",
      "Loss at step 90000: 0.114670\n",
      "Validation at step 90000 MSE: 0.1042\n",
      "Loss at step 91000: 0.055600\n",
      "Loss at step 92000: 0.077755\n",
      "Loss at step 93000: 0.116786\n",
      "Loss at step 94000: 0.078910\n",
      "Loss at step 95000: 0.076514\n",
      "Validation at step 95000 MSE: 0.1031\n",
      "Loss at step 96000: 0.100718\n",
      "Loss at step 97000: 0.087808\n",
      "Loss at step 98000: 0.172567\n",
      "Loss at step 99000: 0.053557\n",
      "Loss at step 100000: 0.117943\n",
      "Validation at step 100000 MSE: 0.1011\n",
      "Loss at step 101000: 0.124884\n",
      "Loss at step 102000: 0.122363\n",
      "Loss at step 103000: 0.060361\n",
      "Loss at step 104000: 0.073005\n",
      "Loss at step 105000: 0.117297\n",
      "Validation at step 105000 MSE: 0.1007\n",
      "Loss at step 106000: 0.067897\n",
      "Loss at step 107000: 0.073007\n",
      "Loss at step 108000: 0.090011\n",
      "Loss at step 109000: 0.084364\n",
      "Loss at step 110000: 0.159386\n",
      "Validation at step 110000 MSE: 0.1015\n",
      "Loss at step 111000: 0.047670\n",
      "Loss at step 112000: 0.097778\n",
      "Loss at step 113000: 0.109640\n",
      "Loss at step 114000: 0.121511\n",
      "Loss at step 115000: 0.075797\n",
      "Validation at step 115000 MSE: 0.1010\n",
      "Loss at step 116000: 0.051451\n",
      "Loss at step 117000: 0.106918\n",
      "Loss at step 118000: 0.066654\n",
      "Loss at step 119000: 0.070608\n",
      "Loss at step 120000: 0.108234\n",
      "Validation at step 120000 MSE: 0.1010\n",
      "Loss at step 121000: 0.081032\n",
      "Loss at step 122000: 0.159217\n",
      "Loss at step 123000: 0.043024\n",
      "Loss at step 124000: 0.122419\n",
      "Loss at step 125000: 0.101592\n",
      "Validation at step 125000 MSE: 0.1005\n",
      "Loss at step 126000: 0.117917\n",
      "Loss at step 127000: 0.074288\n",
      "Loss at step 128000: 0.052083\n",
      "Loss at step 129000: 0.096286\n",
      "Loss at step 130000: 0.055371\n",
      "Validation at step 130000 MSE: 0.0996\n",
      "Loss at step 131000: 0.060483\n",
      "Loss at step 132000: 0.101782\n",
      "Loss at step 133000: 0.065639\n",
      "Loss at step 134000: 0.158962\n",
      "Loss at step 135000: 0.043288\n",
      "Validation at step 135000 MSE: 0.0985\n",
      "Loss at step 136000: 0.085141\n",
      "Loss at step 137000: 0.103949\n",
      "Loss at step 138000: 0.092623\n",
      "Loss at step 139000: 0.064062\n",
      "Loss at step 140000: 0.062268\n",
      "Validation at step 140000 MSE: 0.0990\n",
      "Loss at step 141000: 0.093619\n",
      "Loss at step 142000: 0.048024\n",
      "Loss at step 143000: 0.053442\n",
      "Loss at step 144000: 0.086909\n",
      "Loss at step 145000: 0.066341\n",
      "Validation at step 145000 MSE: 0.0981\n",
      "Loss at step 146000: 0.150976\n",
      "Loss at step 147000: 0.047991\n",
      "Loss at step 148000: 0.074240\n",
      "Loss at step 149000: 0.103761\n",
      "Loss at step 150000: 0.093061\n",
      "Validation at step 150000 MSE: 0.1003\n",
      "Loss at step 151000: 0.073082\n",
      "Loss at step 152000: 0.057504\n",
      "Loss at step 153000: 0.082484\n",
      "Loss at step 154000: 0.043879\n",
      "Loss at step 155000: 0.059393\n",
      "Validation at step 155000 MSE: 0.0981\n",
      "Loss at step 156000: 0.085776\n",
      "Loss at step 157000: 0.067909\n",
      "Loss at step 158000: 0.144841\n",
      "Loss at step 159000: 0.045688\n",
      "Loss at step 160000: 0.069963\n",
      "Validation at step 160000 MSE: 0.0981\n",
      "Loss at step 161000: 0.105487\n",
      "Loss at step 162000: 0.082975\n",
      "Loss at step 163000: 0.067407\n",
      "Loss at step 164000: 0.061303\n",
      "Loss at step 165000: 0.087415\n",
      "Validation at step 165000 MSE: 0.0984\n",
      "Loss at step 166000: 0.040196\n",
      "Loss at step 167000: 0.060472\n",
      "Loss at step 168000: 0.084579\n",
      "Loss at step 169000: 0.065884\n",
      "Loss at step 170000: 0.157370\n",
      "Validation at step 170000 MSE: 0.1009\n",
      "Loss at step 171000: 0.057104\n",
      "Loss at step 172000: 0.071742\n",
      "Loss at step 173000: 0.099822\n",
      "Loss at step 174000: 0.078498\n",
      "Loss at step 175000: 0.065762\n",
      "Validation at step 175000 MSE: 0.1011\n",
      "Loss at step 176000: 0.064978\n",
      "Loss at step 177000: 0.070451\n",
      "Loss at step 178000: 0.036353\n",
      "Loss at step 179000: 0.064270\n",
      "Loss at step 180000: 0.066844\n",
      "Validation at step 180000 MSE: 0.0990\n",
      "Loss at step 181000: 0.073001\n",
      "Loss at step 182000: 0.151725\n",
      "Loss at step 183000: 0.055708\n",
      "Loss at step 184000: 0.069698\n",
      "Loss at step 185000: 0.090829\n",
      "Validation at step 185000 MSE: 0.1015\n",
      "Loss at step 186000: 0.090399\n",
      "Loss at step 187000: 0.069643\n",
      "Loss at step 188000: 0.065193\n",
      "Loss at step 189000: 0.059875\n",
      "Loss at step 190000: 0.037110\n",
      "Validation at step 190000 MSE: 0.0988\n",
      "Loss at step 191000: 0.069414\n",
      "Loss at step 192000: 0.059225\n",
      "Loss at step 193000: 0.059492\n",
      "Loss at step 194000: 0.140967\n",
      "Loss at step 195000: 0.039341\n",
      "Validation at step 195000 MSE: 0.1012\n",
      "Loss at step 196000: 0.091335\n",
      "Loss at step 197000: 0.054643\n",
      "Loss at step 198000: 0.196368\n",
      "Loss at step 199000: 0.073245\n",
      "Loss at step 200000: 0.064259\n",
      "Validation at step 200000 MSE: 0.1013\n",
      "Loss at step 201000: 0.059590\n",
      "Loss at step 202000: 0.042531\n",
      "Loss at step 203000: 0.071489\n",
      "Loss at step 204000: 0.053679\n",
      "Loss at step 205000: 0.061268\n",
      "Validation at step 205000 MSE: 0.0995\n",
      "Loss at step 206000: 0.098820\n",
      "Loss at step 207000: 0.044458\n",
      "Loss at step 208000: 0.091397\n",
      "Loss at step 209000: 0.068898\n",
      "Loss at step 210000: 0.201364\n",
      "Validation at step 210000 MSE: 0.1026\n",
      "Loss at step 211000: 0.064687\n",
      "Loss at step 212000: 0.062500\n",
      "Loss at step 213000: 0.062674\n",
      "Loss at step 214000: 0.053807\n",
      "Loss at step 215000: 0.074221\n",
      "Validation at step 215000 MSE: 0.0995\n",
      "Loss at step 216000: 0.057915\n",
      "Loss at step 217000: 0.059700\n",
      "Loss at step 218000: 0.063968\n",
      "Loss at step 219000: 0.051020\n",
      "Loss at step 220000: 0.092561\n",
      "Validation at step 220000 MSE: 0.0985\n",
      "Loss at step 221000: 0.059083\n",
      "Loss at step 222000: 0.195378\n",
      "Loss at step 223000: 0.056955\n",
      "Loss at step 224000: 0.065245\n",
      "Loss at step 225000: 0.051332\n",
      "Validation at step 225000 MSE: 0.0982\n",
      "Loss at step 226000: 0.061359\n",
      "Loss at step 227000: 0.080440\n",
      "Loss at step 228000: 0.063261\n",
      "Loss at step 229000: 0.138304\n",
      "Loss at step 230000: 0.080323\n",
      "Validation at step 230000 MSE: 0.1031\n",
      "Loss at step 231000: 0.048117\n",
      "Loss at step 232000: 0.088525\n",
      "Loss at step 233000: 0.096431\n",
      "Loss at step 234000: 0.192608\n",
      "Loss at step 235000: 0.076149\n",
      "Validation at step 235000 MSE: 0.0991\n",
      "Loss at step 236000: 0.072227\n",
      "Loss at step 237000: 0.052741\n",
      "Loss at step 238000: 0.061609\n",
      "Loss at step 239000: 0.074324\n",
      "Loss at step 240000: 0.054048\n",
      "Validation at step 240000 MSE: 0.0964\n",
      "Loss at step 241000: 0.129144\n",
      "Loss at step 242000: 0.066204\n",
      "Loss at step 243000: 0.050423\n",
      "Loss at step 244000: 0.065801\n",
      "Loss at step 245000: 0.109897\n",
      "Validation at step 245000 MSE: 0.0976\n",
      "Loss at step 246000: 0.193742\n",
      "Loss at step 247000: 0.074279\n",
      "Loss at step 248000: 0.069412\n",
      "Loss at step 249000: 0.053570\n",
      "Loss at step 250000: 0.069682\n",
      "Validation at step 250000 MSE: 0.0964\n",
      "Loss at step 251000: 0.080966\n",
      "Loss at step 252000: 0.053463\n",
      "Loss at step 253000: 0.124996\n",
      "Loss at step 254000: 0.063899\n",
      "Loss at step 255000: 0.057276\n",
      "Validation at step 255000 MSE: 0.0980\n",
      "Loss at step 256000: 0.056529\n",
      "Loss at step 257000: 0.105513\n",
      "Loss at step 258000: 0.187747\n",
      "Loss at step 259000: 0.080538\n",
      "Loss at step 260000: 0.057676\n",
      "Validation at step 260000 MSE: 0.0978\n",
      "Loss at step 261000: 0.056596\n",
      "Loss at step 262000: 0.067377\n",
      "Loss at step 263000: 0.079610\n",
      "Loss at step 264000: 0.061584\n",
      "Loss at step 265000: 0.077366\n",
      "Validation at step 265000 MSE: 0.0959\n",
      "Loss at step 266000: 0.061185\n",
      "Loss at step 267000: 0.050207\n",
      "Loss at step 268000: 0.052238\n",
      "Loss at step 269000: 0.100098\n",
      "Loss at step 270000: 0.185318\n",
      "Validation at step 270000 MSE: 0.1025\n",
      "Loss at step 271000: 0.075236\n",
      "Loss at step 272000: 0.056755\n",
      "Loss at step 273000: 0.054332\n",
      "Loss at step 274000: 0.075726\n",
      "Loss at step 275000: 0.073576\n",
      "Validation at step 275000 MSE: 0.0956\n",
      "Loss at step 276000: 0.059927\n",
      "Loss at step 277000: 0.085809\n",
      "Loss at step 278000: 0.052579\n",
      "Loss at step 279000: 0.049027\n",
      "Loss at step 280000: 0.050166\n",
      "Validation at step 280000 MSE: 0.0947\n",
      "Loss at step 281000: 0.090864\n",
      "Loss at step 282000: 0.188637\n",
      "Loss at step 283000: 0.074652\n",
      "Loss at step 284000: 0.043724\n",
      "Loss at step 285000: 0.050306\n",
      "Validation at step 285000 MSE: 0.0956\n",
      "Loss at step 286000: 0.083513\n",
      "Loss at step 287000: 0.061373\n",
      "Loss at step 288000: 0.056979\n",
      "Loss at step 289000: 0.094196\n",
      "Loss at step 290000: 0.048512\n",
      "Validation at step 290000 MSE: 0.0991\n",
      "Loss at step 291000: 0.048277\n",
      "Loss at step 292000: 0.062654\n",
      "Loss at step 293000: 0.090276\n",
      "Loss at step 294000: 0.225391\n",
      "Loss at step 295000: 0.078195\n",
      "Validation at step 295000 MSE: 0.0955\n",
      "Loss at step 296000: 0.036643\n",
      "Loss at step 297000: 0.046347\n",
      "Loss at step 298000: 0.082542\n",
      "Loss at step 299000: 0.059484\n",
      "Loss at step 300000: 0.063962\n",
      "Validation at step 300000 MSE: 0.0939\n",
      "Test MSE: 0.1234\n",
      "[[ 1.         0.9355373]\n",
      " [ 0.9355373  1.       ]]\n",
      "[[ 1.          0.95925927]\n",
      " [ 0.95925927  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_output.shape[0] - batch_size)\n",
    "        batch_data = train_dataset2[offset:(offset + batch_size), :]\n",
    "        batch_output = train_output[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_output}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction],feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "        if (step % 5000 == 0):\n",
    "            # print('Training MSE: %.4f' % accuracy_rmse(predictions, train_output))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation at step %d MSE: %.4f' % (step,accuracy_mse(valid_prediction.eval(), valid_output)))\n",
    "    print('Test MSE: %.4f' % accuracy_mse(test_prediction.eval(), test_output))\n",
    "    predicted_vs_actual = np.hstack((test_prediction.eval(), test_output))\n",
    "\n",
    "print(np.corrcoef(predicted_vs_actual[:,0],predicted_vs_actual[:,2]))\n",
    "print(np.corrcoef(predicted_vs_actual[:,1],predicted_vs_actual[:,3]))\n",
    "\n",
    "#plt.plot(predicted_vs_actual[:1000,0],label=\"predicted\")\n",
    "#plt.plot(predicted_vs_actual[:1000,1],label=\"actual\")\n",
    "#plt.ylabel(\"normalized output\")\n",
    "#plt.legend(loc='upper right', shadow=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "stop = 193*10\n",
    "\n",
    "plt.plot(predicted_vs_actual[start:stop,0],label=\"predicted 1\")\n",
    "plt.plot(predicted_vs_actual[start:stop,2],label=\"actual 1\")\n",
    "plt.ylabel(\"normalized output\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(predicted_vs_actual[start:stop,1],label=\"predicted 2\")\n",
    "plt.plot(predicted_vs_actual[start:stop,3],label=\"actual 2\")\n",
    "plt.ylabel(\"normalized output\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e7a600e02f74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#print(graph)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'weights_1' is not defined"
     ]
    }
   ],
   "source": [
    "#print(graph)\n",
    "print(weights_1.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep ANN\n",
    "batch_size = 5*193\n",
    "hidden_nodes_1 = 30\n",
    "hidden_nodes_2 = 15\n",
    "hidden_nodes_3 = 8\n",
    "\n",
    "num_steps = 500001\n",
    "starter_learning_rate = 0.001\n",
    "#starter_learning_rate = 0.005\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 6)) #train_dataset2.shape(2)\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 2))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset2)\n",
    "    tf_test_dataset = tf.constant(test_dataset2)\n",
    "  \n",
    "    weights_0 = tf.Variable(tf.truncated_normal([6,hidden_nodes_1], dtype = tf.float32))\n",
    "    biases_0 = tf.Variable(tf.zeros([hidden_nodes_1], dtype = tf.float32))\n",
    "    \n",
    "    weights_1 = tf.Variable(tf.truncated_normal([hidden_nodes_1,hidden_nodes_2], dtype = tf.float32))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes_2], dtype = tf.float32))\n",
    "    \n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_2,hidden_nodes_3], dtype = tf.float32))\n",
    "    biases_2 = tf.Variable(tf.zeros([hidden_nodes_3], dtype = tf.float32))\n",
    "    \n",
    "    weights_3 = tf.Variable(tf.truncated_normal([hidden_nodes_3,2], dtype = tf.float32))\n",
    "    biases_3 = tf.Variable(tf.zeros([2], dtype = tf.float32))\n",
    "\n",
    "  \n",
    "    input_layer_output = tf.sigmoid(tf.matmul(tf_train_dataset, weights_0) + biases_0)\n",
    "    hidden_layer_output = tf.sigmoid(tf.matmul(input_layer_output, weights_1) + biases_1)\n",
    "    #hidden_layer_output = tf.nn.dropout(hidden_layer_output, 0.5)\n",
    "    hidden_layer_output = tf.sigmoid(tf.matmul(hidden_layer_output, weights_2) + biases_2)\n",
    "    hidden_layer_output = tf.matmul(hidden_layer_output, weights_3) + biases_3\n",
    "    \n",
    "    \n",
    "    loss = tf.cast(tf.reduce_mean(tf.reduce_mean(tf.square(hidden_layer_output-tf_train_labels))),tf.float32)\n",
    "    #loss = tf.cast(tf.reduce_mean(tf.reduce_mean(tf.square(tf.square(hidden_layer_output-tf_train_labels)))),tf.float32)\n",
    "        \n",
    "    global_step = tf.Variable(0.00, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, num_steps, 0.96, staircase=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #starter_learning_rate = 0.5\n",
    "    #optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "    train_prediction = loss\n",
    "    valid_prediction = tf.sigmoid(tf.matmul(tf_valid_dataset, weights_0) + biases_0)\n",
    "    valid_prediction = tf.sigmoid(tf.matmul(valid_prediction, weights_1) + biases_1)\n",
    "    valid_prediction = tf.sigmoid(tf.matmul(valid_prediction, weights_2) + biases_2)\n",
    "    valid_prediction = tf.matmul(valid_prediction, weights_3) + biases_3\n",
    "    \n",
    "    test_prediction = tf.sigmoid(tf.matmul(tf_test_dataset, weights_0) + biases_0)\n",
    "    test_prediction = tf.sigmoid(tf.matmul(test_prediction, weights_1) + biases_1)\n",
    "    test_prediction = tf.sigmoid(tf.matmul(test_prediction, weights_2) + biases_2)\n",
    "    test_prediction = tf.matmul(test_prediction, weights_3) + biases_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 0.939744\n",
      "Validation at step 0 MSE: 0.9555\n",
      "Loss at step 1000: 0.950243\n",
      "Loss at step 2000: 1.169626\n",
      "Loss at step 3000: 0.649897\n",
      "Loss at step 4000: 0.851197\n",
      "Loss at step 5000: 1.066481\n",
      "Validation at step 5000 MSE: 0.8906\n",
      "Loss at step 6000: 1.193856\n",
      "Loss at step 7000: 0.655207\n",
      "Loss at step 8000: 0.684545\n",
      "Loss at step 9000: 0.636956\n",
      "Loss at step 10000: 0.634905\n",
      "Validation at step 10000 MSE: 0.6848\n",
      "Loss at step 11000: 0.557994\n",
      "Loss at step 12000: 0.592374\n",
      "Loss at step 13000: 0.678698\n",
      "Loss at step 14000: 0.552718\n",
      "Loss at step 15000: 0.528755\n",
      "Validation at step 15000 MSE: 0.5761\n",
      "Loss at step 16000: 0.630882\n",
      "Loss at step 17000: 0.791478\n",
      "Loss at step 18000: 0.508274\n",
      "Loss at step 19000: 0.571615\n",
      "Loss at step 20000: 0.569439\n",
      "Validation at step 20000 MSE: 0.5112\n",
      "Loss at step 21000: 0.626798\n",
      "Loss at step 22000: 0.346913\n",
      "Loss at step 23000: 0.484978\n",
      "Loss at step 24000: 0.623188\n",
      "Loss at step 25000: 0.522191\n",
      "Validation at step 25000 MSE: 0.4781\n",
      "Loss at step 26000: 0.457543\n",
      "Loss at step 27000: 0.427914\n",
      "Loss at step 28000: 0.609261\n",
      "Loss at step 29000: 0.410983\n",
      "Loss at step 30000: 0.554003\n",
      "Validation at step 30000 MSE: 0.4602\n",
      "Loss at step 31000: 0.422256\n",
      "Loss at step 32000: 0.911176\n",
      "Loss at step 33000: 0.322213\n",
      "Loss at step 34000: 0.607840\n",
      "Loss at step 35000: 0.254074\n",
      "Validation at step 35000 MSE: 0.4461\n",
      "Loss at step 36000: 1.025989\n",
      "Loss at step 37000: 0.333392\n",
      "Loss at step 38000: 0.550047\n",
      "Loss at step 39000: 0.298615\n",
      "Loss at step 40000: 0.939875\n",
      "Validation at step 40000 MSE: 0.4292\n",
      "Loss at step 41000: 0.400959\n",
      "Loss at step 42000: 0.551761\n",
      "Loss at step 43000: 0.250835\n",
      "Loss at step 44000: 0.525708\n",
      "Loss at step 45000: 0.468952\n",
      "Validation at step 45000 MSE: 0.4043\n",
      "Loss at step 46000: 0.368086\n",
      "Loss at step 47000: 0.501763\n",
      "Loss at step 48000: 0.334301\n",
      "Loss at step 49000: 0.506559\n",
      "Loss at step 50000: 0.271006\n",
      "Validation at step 50000 MSE: 0.3759\n",
      "Loss at step 51000: 0.558446\n",
      "Loss at step 52000: 0.199979\n",
      "Loss at step 53000: 0.376158\n",
      "Loss at step 54000: 0.254864\n",
      "Loss at step 55000: 0.474347\n",
      "Validation at step 55000 MSE: 0.3435\n",
      "Loss at step 56000: 0.284060\n",
      "Loss at step 57000: 0.295368\n",
      "Loss at step 58000: 0.282722\n",
      "Loss at step 59000: 0.529929\n",
      "Loss at step 60000: 0.307246\n",
      "Validation at step 60000 MSE: 0.3107\n",
      "Loss at step 61000: 0.253111\n",
      "Loss at step 62000: 0.232614\n",
      "Loss at step 63000: 0.312465\n",
      "Loss at step 64000: 0.296417\n",
      "Loss at step 65000: 0.254808\n",
      "Validation at step 65000 MSE: 0.2789\n",
      "Loss at step 66000: 0.309629\n",
      "Loss at step 67000: 0.265579\n",
      "Loss at step 68000: 0.273286\n",
      "Loss at step 69000: 0.245063\n",
      "Loss at step 70000: 0.355952\n",
      "Validation at step 70000 MSE: 0.2549\n",
      "Loss at step 71000: 0.261964\n",
      "Loss at step 72000: 0.245058\n",
      "Loss at step 73000: 0.185006\n",
      "Loss at step 74000: 0.318097\n",
      "Loss at step 75000: 0.145863\n",
      "Validation at step 75000 MSE: 0.2367\n",
      "Loss at step 76000: 0.216677\n",
      "Loss at step 77000: 0.243825\n",
      "Loss at step 78000: 0.238443\n",
      "Loss at step 79000: 0.206845\n",
      "Loss at step 80000: 0.168691\n",
      "Validation at step 80000 MSE: 0.2237\n",
      "Loss at step 81000: 0.266664\n",
      "Loss at step 82000: 0.185907\n",
      "Loss at step 83000: 0.263494\n",
      "Loss at step 84000: 0.164118\n",
      "Loss at step 85000: 0.363623\n",
      "Validation at step 85000 MSE: 0.2133\n",
      "Loss at step 86000: 0.130558\n",
      "Loss at step 87000: 0.276880\n",
      "Loss at step 88000: 0.138864\n",
      "Loss at step 89000: 0.342480\n",
      "Loss at step 90000: 0.133411\n",
      "Validation at step 90000 MSE: 0.2048\n",
      "Loss at step 91000: 0.260807\n",
      "Loss at step 92000: 0.147315\n",
      "Loss at step 93000: 0.298900\n",
      "Loss at step 94000: 0.166162\n",
      "Loss at step 95000: 0.254689\n",
      "Validation at step 95000 MSE: 0.1988\n",
      "Loss at step 96000: 0.151307\n",
      "Loss at step 97000: 0.159629\n",
      "Loss at step 98000: 0.218340\n",
      "Loss at step 99000: 0.167570\n",
      "Loss at step 100000: 0.214701\n",
      "Validation at step 100000 MSE: 0.1935\n",
      "Loss at step 101000: 0.124834\n",
      "Loss at step 102000: 0.248900\n",
      "Loss at step 103000: 0.144658\n",
      "Loss at step 104000: 0.238649\n",
      "Loss at step 105000: 0.089248\n",
      "Validation at step 105000 MSE: 0.1900\n",
      "Loss at step 106000: 0.177663\n",
      "Loss at step 107000: 0.122729\n",
      "Loss at step 108000: 0.228209\n",
      "Loss at step 109000: 0.100589\n",
      "Loss at step 110000: 0.182574\n",
      "Validation at step 110000 MSE: 0.1854\n",
      "Loss at step 111000: 0.132554\n",
      "Loss at step 112000: 0.290368\n",
      "Loss at step 113000: 0.131960\n",
      "Loss at step 114000: 0.148130\n",
      "Loss at step 115000: 0.112732\n",
      "Validation at step 115000 MSE: 0.1819\n",
      "Loss at step 116000: 0.208219\n",
      "Loss at step 117000: 0.145417\n",
      "Loss at step 118000: 0.155624\n",
      "Loss at step 119000: 0.202867\n",
      "Loss at step 120000: 0.181647\n",
      "Validation at step 120000 MSE: 0.1792\n",
      "Loss at step 121000: 0.160965\n",
      "Loss at step 122000: 0.146340\n",
      "Loss at step 123000: 0.224647\n",
      "Loss at step 124000: 0.190738\n",
      "Loss at step 125000: 0.143230\n",
      "Validation at step 125000 MSE: 0.1770\n",
      "Loss at step 126000: 0.108607\n",
      "Loss at step 127000: 0.216134\n",
      "Loss at step 128000: 0.098776\n",
      "Loss at step 129000: 0.149042\n",
      "Loss at step 130000: 0.169532\n",
      "Validation at step 130000 MSE: 0.1741\n",
      "Loss at step 131000: 0.157402\n",
      "Loss at step 132000: 0.141528\n",
      "Loss at step 133000: 0.122158\n",
      "Loss at step 134000: 0.192392\n",
      "Loss at step 135000: 0.121760\n",
      "Validation at step 135000 MSE: 0.1710\n",
      "Loss at step 136000: 0.186181\n",
      "Loss at step 137000: 0.122994\n",
      "Loss at step 138000: 0.238795\n",
      "Loss at step 139000: 0.080388\n",
      "Loss at step 140000: 0.199912\n",
      "Validation at step 140000 MSE: 0.1690\n",
      "Loss at step 141000: 0.109360\n",
      "Loss at step 142000: 0.222509\n",
      "Loss at step 143000: 0.081289\n",
      "Loss at step 144000: 0.198893\n",
      "Loss at step 145000: 0.111548\n",
      "Validation at step 145000 MSE: 0.1661\n",
      "Loss at step 146000: 0.192914\n",
      "Loss at step 147000: 0.106384\n",
      "Loss at step 148000: 0.199187\n",
      "Loss at step 149000: 0.126365\n",
      "Loss at step 150000: 0.116349\n",
      "Validation at step 150000 MSE: 0.1646\n",
      "Loss at step 151000: 0.155612\n",
      "Loss at step 152000: 0.133382\n",
      "Loss at step 153000: 0.180774\n",
      "Loss at step 154000: 0.092579\n",
      "Loss at step 155000: 0.183782\n",
      "Validation at step 155000 MSE: 0.1611\n",
      "Loss at step 156000: 0.118619\n",
      "Loss at step 157000: 0.208824\n",
      "Loss at step 158000: 0.069664\n",
      "Loss at step 159000: 0.140988\n",
      "Loss at step 160000: 0.104126\n",
      "Validation at step 160000 MSE: 0.1590\n",
      "Loss at step 161000: 0.203395\n",
      "Loss at step 162000: 0.077005\n",
      "Loss at step 163000: 0.144297\n",
      "Loss at step 164000: 0.115365\n",
      "Loss at step 165000: 0.255976\n",
      "Validation at step 165000 MSE: 0.1568\n",
      "Loss at step 166000: 0.103623\n",
      "Loss at step 167000: 0.122987\n",
      "Loss at step 168000: 0.101528\n",
      "Loss at step 169000: 0.187900\n",
      "Loss at step 170000: 0.115959\n",
      "Validation at step 170000 MSE: 0.1552\n",
      "Loss at step 171000: 0.131693\n",
      "Loss at step 172000: 0.178270\n",
      "Loss at step 173000: 0.163899\n",
      "Loss at step 174000: 0.137659\n",
      "Loss at step 175000: 0.123653\n",
      "Validation at step 175000 MSE: 0.1526\n",
      "Loss at step 176000: 0.195463\n",
      "Loss at step 177000: 0.172267\n",
      "Loss at step 178000: 0.123070\n",
      "Loss at step 179000: 0.093848\n",
      "Loss at step 180000: 0.182608\n",
      "Validation at step 180000 MSE: 0.1504\n",
      "Loss at step 181000: 0.088045\n",
      "Loss at step 182000: 0.136630\n",
      "Loss at step 183000: 0.145823\n",
      "Loss at step 184000: 0.129544\n",
      "Loss at step 185000: 0.119402\n",
      "Validation at step 185000 MSE: 0.1485\n",
      "Loss at step 186000: 0.112975\n",
      "Loss at step 187000: 0.163506\n",
      "Loss at step 188000: 0.096656\n",
      "Loss at step 189000: 0.154897\n",
      "Loss at step 190000: 0.108961\n",
      "Validation at step 190000 MSE: 0.1467\n",
      "Loss at step 191000: 0.193311\n",
      "Loss at step 192000: 0.061743\n",
      "Loss at step 193000: 0.165279\n",
      "Loss at step 194000: 0.095162\n",
      "Loss at step 195000: 0.179022\n",
      "Validation at step 195000 MSE: 0.1462\n",
      "Loss at step 196000: 0.062463\n",
      "Loss at step 197000: 0.165839\n",
      "Loss at step 198000: 0.094606\n",
      "Loss at step 199000: 0.156083\n",
      "Loss at step 200000: 0.083277\n",
      "Validation at step 200000 MSE: 0.1434\n",
      "Loss at step 201000: 0.168345\n",
      "Loss at step 202000: 0.110795\n",
      "Loss at step 203000: 0.097635\n",
      "Loss at step 204000: 0.128994\n",
      "Loss at step 205000: 0.112917\n",
      "Validation at step 205000 MSE: 0.1422\n",
      "Loss at step 206000: 0.161955\n",
      "Loss at step 207000: 0.079585\n",
      "Loss at step 208000: 0.154471\n",
      "Loss at step 209000: 0.101107\n",
      "Loss at step 210000: 0.191076\n",
      "Validation at step 210000 MSE: 0.1404\n",
      "Loss at step 211000: 0.060585\n",
      "Loss at step 212000: 0.127578\n",
      "Loss at step 213000: 0.092594\n",
      "Loss at step 214000: 0.186472\n",
      "Loss at step 215000: 0.067227\n",
      "Validation at step 215000 MSE: 0.1400\n",
      "Loss at step 216000: 0.126404\n",
      "Loss at step 217000: 0.102306\n",
      "Loss at step 218000: 0.235487\n",
      "Loss at step 219000: 0.091650\n",
      "Loss at step 220000: 0.110067\n",
      "Validation at step 220000 MSE: 0.1378\n",
      "Loss at step 221000: 0.093683\n",
      "Loss at step 222000: 0.174967\n",
      "Loss at step 223000: 0.102768\n",
      "Loss at step 224000: 0.117268\n",
      "Loss at step 225000: 0.162414\n",
      "Validation at step 225000 MSE: 0.1364\n",
      "Loss at step 226000: 0.152126\n",
      "Loss at step 227000: 0.126083\n",
      "Loss at step 228000: 0.109242\n",
      "Loss at step 229000: 0.178860\n",
      "Loss at step 230000: 0.159909\n",
      "Validation at step 230000 MSE: 0.1351\n",
      "Loss at step 231000: 0.113008\n",
      "Loss at step 232000: 0.082320\n",
      "Loss at step 233000: 0.164742\n",
      "Loss at step 234000: 0.083492\n",
      "Loss at step 235000: 0.131175\n",
      "Validation at step 235000 MSE: 0.1344\n",
      "Loss at step 236000: 0.130562\n",
      "Loss at step 237000: 0.115829\n",
      "Loss at step 238000: 0.110858\n",
      "Loss at step 239000: 0.107690\n",
      "Loss at step 240000: 0.146749\n",
      "Validation at step 240000 MSE: 0.1336\n",
      "Loss at step 241000: 0.085284\n",
      "Loss at step 242000: 0.139800\n",
      "Loss at step 243000: 0.102031\n",
      "Loss at step 244000: 0.170635\n",
      "Loss at step 245000: 0.054041\n",
      "Validation at step 245000 MSE: 0.1321\n",
      "Loss at step 246000: 0.147356\n",
      "Loss at step 247000: 0.089893\n",
      "Loss at step 248000: 0.156817\n",
      "Loss at step 249000: 0.054869\n",
      "Loss at step 250000: 0.147043\n",
      "Validation at step 250000 MSE: 0.1314\n",
      "Loss at step 251000: 0.086119\n",
      "Loss at step 252000: 0.138133\n",
      "Loss at step 253000: 0.073067\n",
      "Loss at step 254000: 0.149430\n",
      "Loss at step 255000: 0.103633\n",
      "Validation at step 255000 MSE: 0.1300\n",
      "Loss at step 256000: 0.086606\n",
      "Loss at step 257000: 0.115609\n",
      "Loss at step 258000: 0.100226\n",
      "Loss at step 259000: 0.150663\n",
      "Loss at step 260000: 0.072817\n",
      "Validation at step 260000 MSE: 0.1300\n",
      "Loss at step 261000: 0.139434\n",
      "Loss at step 262000: 0.089185\n",
      "Loss at step 263000: 0.178972\n",
      "Loss at step 264000: 0.056860\n",
      "Loss at step 265000: 0.120438\n",
      "Validation at step 265000 MSE: 0.1289\n",
      "Loss at step 266000: 0.084332\n",
      "Loss at step 267000: 0.174530\n",
      "Loss at step 268000: 0.063099\n",
      "Loss at step 269000: 0.116307\n",
      "Loss at step 270000: 0.093185\n",
      "Validation at step 270000 MSE: 0.1276\n",
      "Loss at step 271000: 0.221877\n",
      "Loss at step 272000: 0.086017\n",
      "Loss at step 273000: 0.102520\n",
      "Loss at step 274000: 0.087402\n",
      "Loss at step 275000: 0.166100\n",
      "Validation at step 275000 MSE: 0.1266\n",
      "Loss at step 276000: 0.095945\n",
      "Loss at step 277000: 0.108966\n",
      "Loss at step 278000: 0.151733\n",
      "Loss at step 279000: 0.144835\n",
      "Loss at step 280000: 0.119734\n",
      "Validation at step 280000 MSE: 0.1264\n",
      "Loss at step 281000: 0.100855\n",
      "Loss at step 282000: 0.168062\n",
      "Loss at step 283000: 0.152261\n",
      "Loss at step 284000: 0.107093\n",
      "Loss at step 285000: 0.075555\n",
      "Validation at step 285000 MSE: 0.1252\n",
      "Loss at step 286000: 0.154319\n",
      "Loss at step 287000: 0.080294\n",
      "Loss at step 288000: 0.128140\n",
      "Loss at step 289000: 0.121515\n",
      "Loss at step 290000: 0.109024\n",
      "Validation at step 290000 MSE: 0.1245\n",
      "Loss at step 291000: 0.105444\n",
      "Loss at step 292000: 0.104138\n",
      "Loss at step 293000: 0.137298\n",
      "Loss at step 294000: 0.079941\n",
      "Loss at step 295000: 0.130636\n",
      "Validation at step 295000 MSE: 0.1241\n",
      "Loss at step 296000: 0.098325\n",
      "Loss at step 297000: 0.158461\n",
      "Loss at step 298000: 0.050735\n",
      "Loss at step 299000: 0.136125\n",
      "Loss at step 300000: 0.087465\n",
      "Validation at step 300000 MSE: 0.1230\n",
      "Loss at step 301000: 0.144997\n",
      "Loss at step 302000: 0.051279\n",
      "Loss at step 303000: 0.135355\n",
      "Loss at step 304000: 0.081163\n",
      "Loss at step 305000: 0.128415\n",
      "Validation at step 305000 MSE: 0.1237\n",
      "Loss at step 306000: 0.068011\n",
      "Loss at step 307000: 0.138471\n",
      "Loss at step 308000: 0.099808\n",
      "Loss at step 309000: 0.080053\n",
      "Loss at step 310000: 0.107636\n",
      "Validation at step 310000 MSE: 0.1219\n",
      "Loss at step 311000: 0.092909\n",
      "Loss at step 312000: 0.143620\n",
      "Loss at step 313000: 0.068696\n",
      "Loss at step 314000: 0.130566\n",
      "Loss at step 315000: 0.082858\n",
      "Validation at step 315000 MSE: 0.1215\n",
      "Loss at step 316000: 0.170661\n",
      "Loss at step 317000: 0.054820\n",
      "Loss at step 318000: 0.115412\n",
      "Loss at step 319000: 0.080258\n",
      "Loss at step 320000: 0.166326\n",
      "Validation at step 320000 MSE: 0.1207\n",
      "Loss at step 321000: 0.061236\n",
      "Loss at step 322000: 0.109940\n",
      "Loss at step 323000: 0.087665\n",
      "Loss at step 324000: 0.211915\n",
      "Loss at step 325000: 0.083164\n",
      "Validation at step 325000 MSE: 0.1220\n",
      "Loss at step 326000: 0.097572\n",
      "Loss at step 327000: 0.083274\n",
      "Loss at step 328000: 0.159332\n",
      "Loss at step 329000: 0.092089\n",
      "Loss at step 330000: 0.104100\n",
      "Validation at step 330000 MSE: 0.1198\n",
      "Loss at step 331000: 0.143795\n",
      "Loss at step 332000: 0.139003\n",
      "Loss at step 333000: 0.115354\n",
      "Loss at step 334000: 0.095491\n",
      "Loss at step 335000: 0.160488\n",
      "Validation at step 335000 MSE: 0.1194\n",
      "Loss at step 336000: 0.145977\n",
      "Loss at step 337000: 0.102482\n",
      "Loss at step 338000: 0.071315\n",
      "Loss at step 339000: 0.147257\n",
      "Loss at step 340000: 0.077171\n",
      "Validation at step 340000 MSE: 0.1188\n",
      "Loss at step 341000: 0.125694\n",
      "Loss at step 342000: 0.115693\n",
      "Loss at step 343000: 0.105083\n",
      "Loss at step 344000: 0.100433\n",
      "Loss at step 345000: 0.101516\n",
      "Validation at step 345000 MSE: 0.1184\n",
      "Loss at step 346000: 0.131445\n",
      "Loss at step 347000: 0.076815\n",
      "Loss at step 348000: 0.123892\n",
      "Loss at step 349000: 0.095698\n",
      "Loss at step 350000: 0.151247\n",
      "Validation at step 350000 MSE: 0.1187\n",
      "Loss at step 351000: 0.048815\n",
      "Loss at step 352000: 0.127868\n",
      "Loss at step 353000: 0.085560\n",
      "Loss at step 354000: 0.138018\n",
      "Loss at step 355000: 0.049028\n",
      "Validation at step 355000 MSE: 0.1178\n",
      "Loss at step 356000: 0.127496\n",
      "Loss at step 357000: 0.077464\n",
      "Loss at step 358000: 0.122713\n",
      "Loss at step 359000: 0.065086\n",
      "Loss at step 360000: 0.132176\n",
      "Validation at step 360000 MSE: 0.1174\n",
      "Loss at step 361000: 0.097247\n",
      "Loss at step 362000: 0.075994\n",
      "Loss at step 363000: 0.102467\n",
      "Loss at step 364000: 0.088943\n",
      "Loss at step 365000: 0.138588\n",
      "Validation at step 365000 MSE: 0.1166\n",
      "Loss at step 366000: 0.065854\n",
      "Loss at step 367000: 0.124835\n",
      "Loss at step 368000: 0.079769\n",
      "Loss at step 369000: 0.164206\n",
      "Loss at step 370000: 0.053466\n",
      "Validation at step 370000 MSE: 0.1172\n",
      "Loss at step 371000: 0.111216\n",
      "Loss at step 372000: 0.078303\n",
      "Loss at step 373000: 0.159999\n",
      "Loss at step 374000: 0.060219\n",
      "Loss at step 375000: 0.105349\n",
      "Validation at step 375000 MSE: 0.1162\n",
      "Loss at step 376000: 0.083963\n",
      "Loss at step 377000: 0.203264\n",
      "Loss at step 378000: 0.081561\n",
      "Loss at step 379000: 0.093795\n",
      "Loss at step 380000: 0.080343\n",
      "Validation at step 380000 MSE: 0.1160\n",
      "Loss at step 381000: 0.153385\n",
      "Loss at step 382000: 0.089635\n",
      "Loss at step 383000: 0.100804\n",
      "Loss at step 384000: 0.137180\n",
      "Loss at step 385000: 0.133785\n",
      "Validation at step 385000 MSE: 0.1153\n",
      "Loss at step 386000: 0.112079\n",
      "Loss at step 387000: 0.091607\n",
      "Loss at step 388000: 0.154456\n",
      "Loss at step 389000: 0.140503\n",
      "Loss at step 390000: 0.098842\n",
      "Validation at step 390000 MSE: 0.1155\n",
      "Loss at step 391000: 0.068396\n",
      "Loss at step 392000: 0.141584\n",
      "Loss at step 393000: 0.074476\n",
      "Loss at step 394000: 0.123710\n",
      "Loss at step 395000: 0.111480\n",
      "Validation at step 395000 MSE: 0.1149\n",
      "Loss at step 396000: 0.102241\n",
      "Loss at step 397000: 0.096103\n",
      "Loss at step 398000: 0.099539\n",
      "Loss at step 399000: 0.127328\n",
      "Loss at step 400000: 0.074589\n",
      "Validation at step 400000 MSE: 0.1146\n",
      "Loss at step 401000: 0.118561\n",
      "Loss at step 402000: 0.093569\n",
      "Loss at step 403000: 0.146383\n",
      "Loss at step 404000: 0.047415\n",
      "Loss at step 405000: 0.121254\n",
      "Validation at step 405000 MSE: 0.1147\n",
      "Loss at step 406000: 0.083883\n",
      "Loss at step 407000: 0.133331\n",
      "Loss at step 408000: 0.047424\n",
      "Loss at step 409000: 0.121731\n",
      "Loss at step 410000: 0.074498\n",
      "Validation at step 410000 MSE: 0.1134\n",
      "Loss at step 411000: 0.119125\n",
      "Loss at step 412000: 0.063218\n",
      "Loss at step 413000: 0.127741\n",
      "Loss at step 414000: 0.095332\n",
      "Loss at step 415000: 0.073345\n",
      "Validation at step 415000 MSE: 0.1144\n",
      "Loss at step 416000: 0.098959\n",
      "Loss at step 417000: 0.086270\n",
      "Loss at step 418000: 0.134678\n",
      "Loss at step 419000: 0.063948\n",
      "Loss at step 420000: 0.120911\n",
      "Validation at step 420000 MSE: 0.1129\n",
      "Loss at step 421000: 0.077766\n",
      "Loss at step 422000: 0.158856\n",
      "Loss at step 423000: 0.052580\n",
      "Loss at step 424000: 0.107972\n",
      "Loss at step 425000: 0.076826\n",
      "Validation at step 425000 MSE: 0.1130\n",
      "Loss at step 426000: 0.154723\n",
      "Loss at step 427000: 0.059563\n",
      "Loss at step 428000: 0.102129\n",
      "Loss at step 429000: 0.081427\n",
      "Loss at step 430000: 0.195606\n",
      "Validation at step 430000 MSE: 0.1126\n",
      "Loss at step 431000: 0.080547\n",
      "Loss at step 432000: 0.090991\n",
      "Loss at step 433000: 0.078240\n",
      "Loss at step 434000: 0.148288\n",
      "Loss at step 435000: 0.087958\n",
      "Validation at step 435000 MSE: 0.1135\n",
      "Loss at step 436000: 0.098412\n",
      "Loss at step 437000: 0.131853\n",
      "Loss at step 438000: 0.129411\n",
      "Loss at step 439000: 0.109678\n",
      "Loss at step 440000: 0.088760\n",
      "Validation at step 440000 MSE: 0.1127\n",
      "Loss at step 441000: 0.149596\n",
      "Loss at step 442000: 0.136050\n",
      "Loss at step 443000: 0.096208\n",
      "Loss at step 444000: 0.066361\n",
      "Loss at step 445000: 0.136836\n",
      "Validation at step 445000 MSE: 0.1120\n",
      "Loss at step 446000: 0.072330\n",
      "Loss at step 447000: 0.122322\n",
      "Loss at step 448000: 0.108323\n",
      "Loss at step 449000: 0.099941\n",
      "Loss at step 450000: 0.092697\n",
      "Validation at step 450000 MSE: 0.1115\n",
      "Loss at step 451000: 0.098138\n",
      "Loss at step 452000: 0.124155\n",
      "Loss at step 453000: 0.072723\n",
      "Loss at step 454000: 0.114323\n",
      "Loss at step 455000: 0.091939\n",
      "Validation at step 455000 MSE: 0.1112\n",
      "Loss at step 456000: 0.142646\n",
      "Loss at step 457000: 0.046244\n",
      "Loss at step 458000: 0.115880\n",
      "Loss at step 459000: 0.082478\n",
      "Loss at step 460000: 0.129830\n",
      "Validation at step 460000 MSE: 0.1123\n",
      "Loss at step 461000: 0.046217\n",
      "Loss at step 462000: 0.117383\n",
      "Loss at step 463000: 0.072063\n",
      "Loss at step 464000: 0.116556\n",
      "Loss at step 465000: 0.061965\n",
      "Validation at step 465000 MSE: 0.1113\n",
      "Loss at step 466000: 0.123924\n",
      "Loss at step 467000: 0.093734\n",
      "Loss at step 468000: 0.071519\n",
      "Loss at step 469000: 0.096475\n",
      "Loss at step 470000: 0.083946\n",
      "Validation at step 470000 MSE: 0.1112\n",
      "Loss at step 471000: 0.131348\n",
      "Loss at step 472000: 0.062722\n",
      "Loss at step 473000: 0.118094\n",
      "Loss at step 474000: 0.075993\n",
      "Loss at step 475000: 0.154414\n",
      "Validation at step 475000 MSE: 0.1102\n",
      "Loss at step 476000: 0.052005\n",
      "Loss at step 477000: 0.105549\n",
      "Loss at step 478000: 0.075215\n",
      "Loss at step 479000: 0.150217\n",
      "Loss at step 480000: 0.058976\n",
      "Validation at step 480000 MSE: 0.1115\n",
      "Loss at step 481000: 0.099977\n",
      "Loss at step 482000: 0.079722\n",
      "Loss at step 483000: 0.189025\n",
      "Loss at step 484000: 0.079799\n",
      "Loss at step 485000: 0.088973\n",
      "Validation at step 485000 MSE: 0.1103\n",
      "Loss at step 486000: 0.076700\n",
      "Loss at step 487000: 0.144083\n",
      "Loss at step 488000: 0.086745\n",
      "Loss at step 489000: 0.096526\n",
      "Loss at step 490000: 0.127747\n",
      "Validation at step 490000 MSE: 0.1104\n",
      "Loss at step 491000: 0.125867\n",
      "Loss at step 492000: 0.107804\n",
      "Loss at step 493000: 0.086568\n",
      "Loss at step 494000: 0.145685\n",
      "Loss at step 495000: 0.132460\n",
      "Validation at step 495000 MSE: 0.1097\n",
      "Loss at step 496000: 0.094288\n",
      "Loss at step 497000: 0.064912\n",
      "Loss at step 498000: 0.132881\n",
      "Loss at step 499000: 0.070627\n",
      "Loss at step 500000: 0.121204\n",
      "Validation at step 500000 MSE: 0.1103\n",
      "Test RMSE: 0.1052\n",
      "[[ 1.          0.93792698]\n",
      " [ 0.93792698  1.        ]]\n",
      "[[ 1.          0.95121515]\n",
      " [ 0.95121515  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_output.shape[0] - batch_size)\n",
    "        batch_data = train_dataset2[offset:(offset + batch_size), :]\n",
    "        batch_output = train_output[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_output}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction],feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "        if (step % 5000 == 0):\n",
    "            # print('Training MSE: %.4f' % accuracy_rmse(predictions, train_output))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation at step %d MSE: %.4f' % (step,accuracy_rmse(valid_prediction.eval(), valid_output)))\n",
    "    print('Test RMSE: %.4f' % accuracy_rmse(test_prediction.eval(), test_output))\n",
    "    predicted_vs_actual = np.hstack((test_prediction.eval(), test_output))\n",
    "\n",
    "print(np.corrcoef(predicted_vs_actual[:,0],predicted_vs_actual[:,2]))\n",
    "print(np.corrcoef(predicted_vs_actual[:,1],predicted_vs_actual[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "stop = 193*10\n",
    "\n",
    "plt.plot(predicted_vs_actual[start:stop,0],label=\"predicted 1\")\n",
    "plt.plot(predicted_vs_actual[start:stop,2],label=\"actual 1\")\n",
    "plt.ylabel(\"normalized output\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(predicted_vs_actual[start:stop,1],label=\"predicted 2\")\n",
    "plt.plot(predicted_vs_actual[start:stop,3],label=\"actual 2\")\n",
    "plt.ylabel(\"normalized output\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
