{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../code')\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import load_datasets as ld\n",
    "import datetime as dt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_boolean('train', True, 'When True, run training & save model. When False, load a previously saved model and evaluate it')\n",
    "\n",
    "# Split the training data into batches. Each hurricane is 193 records. Batch sizes are usually 2^k\n",
    "# When batch size equals to 0, or exceeds available data, use the whole dataset\n",
    "# Large batch sizes produce more accurate update gradients, but the training is slower\n",
    "flags.DEFINE_integer('batch_size', 64*193, 'Batch size. Divides evenly into the dataset size of 193')\n",
    "\n",
    "# Not currently used. The data is loaded in load_datasets (ld) and put in Dataset objects:\n",
    "# train_dataset, valid_dataset, and test_dataset\n",
    "#flags.DEFINE_string('train_dir', './data/', 'Directory to put the training data')\n",
    "\n",
    "# Save models in this directory\n",
    "flags.DEFINE_string('checkpoints_dir', './checkpoints', 'Directory to store checkpoints')\n",
    "\n",
    "# Statistics\n",
    "flags.DEFINE_string('summaries_dir','./logs','Summaries directory')\n",
    "\n",
    "# Evaluation\n",
    "# Output dataset\n",
    "flags.DEFINE_string('output','./test_track_out2.dat','When model evaluation, output the data here')\n",
    "# Input dataset\n",
    "flags.DEFINE_string('input','./test_track.dat','Dataset for input')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/324 \n",
      "\n",
      "Processed 100/324 \n",
      "\n",
      "Processed 200/324 \n",
      "\n",
      "Processed 300/324 \n",
      "\n",
      "calculate new means, stds for dataset with 43811 samples\n",
      "using provided means, stds for dataset with 9457 samples\n",
      "using provided means, stds for dataset with 9264 samples\n",
      "Num hurricanes in train 227, validation 49, test 48\n",
      "Step 0 (16.92 op/sec): Training loss: 0.10945, Validation loss: 0.08301\n",
      "Step 500 (21.52 op/sec): Training loss: 0.01127, Validation loss: 0.01373\n",
      "Step 1000 (19.72 op/sec): Training loss: 0.00698, Validation loss: 0.01084\n",
      "Step 1500 (19.55 op/sec): Training loss: 0.00672, Validation loss: 0.00931\n",
      "Step 2000 (19.55 op/sec): Training loss: 0.00589, Validation loss: 0.00817\n"
     ]
    }
   ],
   "source": [
    "import ilt_three_layers as ilt\n",
    "#import ilt_density as ilt\n",
    "\n",
    "def fill_feed_dict(data_set, inputs_pl, outputs_pl, train):\n",
    "    \"\"\"\n",
    "    Returns feed dictionary for TF. \n",
    "    data_set -- dataset\n",
    "    inputs_pl -- TF placeholder for inputs\n",
    "    outputs_pl -- TF placeholder for outputs\n",
    "    train -- if TRUE, then return DS in batches for training. Otherwise, return complete DS for validation/testing\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        batch_size = FLAGS.batch_size\n",
    "    else:\n",
    "        batch_size = 0\n",
    "\n",
    "    # Read next batch of data from the dataset\n",
    "    inputs, outputs = data_set.next_batch(batch_size = batch_size)\n",
    "\n",
    "    # Create dictionary for return\n",
    "    feed_dict = {\n",
    "        inputs_pl: inputs,\n",
    "        outputs_pl: outputs\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Finish building the graph and run training on a single CPU's\n",
    "    \"\"\"\n",
    "    # Read datasets \n",
    "    train_dataset, valid_dataset, test_dataset = ld.read_data_sets()\n",
    "\n",
    "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "        # Prepare placeholders for inputs and expected outputs\n",
    "        x = tf.placeholder(tf.float32, [None, FLAGS.input_vars], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, FLAGS.output_vars], name = 'y-input')\n",
    "\n",
    "        # Create variables for input data moments and initialize them with train datasets' moments\n",
    "        means = tf.get_variable('means', trainable = False, \n",
    "                                initializer = tf.convert_to_tensor(train_dataset.means))\n",
    "        stds = tf.get_variable('stds', trainable = False, \n",
    "                                initializer = tf.convert_to_tensor(train_dataset.stds))\n",
    "\n",
    "        # Normalize input data\n",
    "        x_normalized = tf.div(tf.sub(x,means),stds)\n",
    "\n",
    "        # Prepare global step and learning rate for optimization\n",
    "        global_step = tf.get_variable(\n",
    "            'global_step', [], \n",
    "            initializer=tf.constant_initializer(0), trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            FLAGS.learning_rate, global_step, FLAGS.max_steps,\n",
    "            FLAGS.learning_rate_decay, staircase=False)        \n",
    "\n",
    "        # Create ADAM optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        outputs = ilt.inference(x_normalized)\n",
    "        loss = ilt.loss(outputs, y_)\n",
    "        tf.scalar_summary('MSE', loss)\n",
    "        #tf.scalar_summary('CC',tf.get_collection('cc')[0])\n",
    "\n",
    "\n",
    "        # Calculate gradients and apply them\n",
    "        grads = optimizer.compute_gradients(loss)\n",
    "        apply_gradient_op = optimizer.apply_gradients(grads, global_step = global_step)\n",
    "\n",
    "        # Smoothen variables after gradient applications\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(\n",
    "            FLAGS.moving_avg_decay, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "        train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "        #train_op = apply_gradient_op\n",
    "\n",
    "        merged = tf.merge_all_summaries()\n",
    "           \n",
    "        init = tf.initialize_all_variables()\n",
    "        sess = tf.Session(config = tf.ConfigProto(\n",
    "            allow_soft_placement = False, # allows to utilize GPU's & CPU's\n",
    "            log_device_placement = False)) # shows GPU/CPU allocation\n",
    "        # Prepare folders for saving models and its stats\n",
    "        date_time_stamp = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        train_writer = tf.train.SummaryWriter(FLAGS.summaries_dir+'/train/'+date_time_stamp) #, sess.graph)\n",
    "        test_writer = tf.train.SummaryWriter(FLAGS.summaries_dir+'/validation/'+date_time_stamp) #, sess.graph)\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Finish graph creation. Below is the code for running graph\n",
    "        sess.run(init)\n",
    "        tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "        valid_loss = 1.0\n",
    "        train_loss = 1.0\n",
    "        step = 1\n",
    "        # Main training loop\n",
    "        for step in xrange(FLAGS.max_steps):\n",
    "            start_time = time.time()\n",
    "            # regular training\n",
    "            \n",
    "            _, train_loss, summary, lr = sess.run(\n",
    "                [train_op, loss, merged, learning_rate], feed_dict=fill_feed_dict(train_dataset, x, y_, train = True))\n",
    "\n",
    "            duration = time.time()-start_time\n",
    "            train_writer.add_summary(summary,step)\n",
    "            if step%(FLAGS.max_steps//20) == 0:\n",
    "                # check model fit\n",
    "                feed_dict = fill_feed_dict(valid_dataset, x, y_, train = False)\n",
    "                valid_loss, summary = sess.run([loss, merged], feed_dict = feed_dict)\n",
    "                test_writer.add_summary(summary,step)\n",
    "                print('Step %d (%.2f op/sec): Training loss: %.5f, Validation loss: %.5f' % (\n",
    "                    step, 1.0/duration, np.float32(train_loss).item(), np.float32(valid_loss).item()))\n",
    "\n",
    "        checkpoint_path = os.path.join(FLAGS.checkpoints_dir,'model.ckpt')\n",
    "        saver.save(sess, checkpoint_path, global_step=step)\n",
    "            \n",
    "        feed_dict = fill_feed_dict(test_dataset, x, y_, train = False)\n",
    "        test_loss = sess.run([loss], feed_dict = feed_dict)\n",
    "        print('Test: %.5f' % (np.float32(test_loss).item()))\n",
    "\n",
    "        outs = outputs.eval(session=sess, feed_dict = feed_dict)\n",
    "\n",
    "        for out_no in range(0,FLAGS.output_vars):\n",
    "            print(\"Location %d: CC: %.4f, MSE: %.6f\"%(\n",
    "                out_no,\n",
    "                np.corrcoef(outs[:,out_no], test_dataset.outputs[:,out_no])[0,1],\n",
    "                  mean_squared_error(outs[:,out_no], test_dataset.outputs[:,out_no])))\n",
    "        sess.close()\n",
    "        return outs, test_dataset.outputs\n",
    "\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Finish building the graph and run it on the default device\n",
    "    \"\"\"\n",
    "    # Assign datasets \n",
    "    test_ds = np.loadtxt(FLAGS.input)[:,1:7].reshape((-1, 6)).astype(np.float32)\n",
    "\n",
    "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "        # Prepare placeholders for inputs and expected outputs\n",
    "        x = tf.placeholder(tf.float32, [None, FLAGS.input_vars], name='x-input')\n",
    "\n",
    "        means = tf.get_variable('means', shape=[FLAGS.input_vars], trainable = False)\n",
    "        stds = tf.get_variable('stds', shape=[FLAGS.input_vars], trainable = False)\n",
    "\n",
    "        # Normalize input data\n",
    "        x_normalized = tf.div(tf.sub(x,means),stds)\n",
    "\n",
    "        outputs = ilt.inference(x_normalized)\n",
    "\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess = tf.Session(config = tf.ConfigProto(\n",
    "            allow_soft_placement = False, # allows to utilize GPU's & CPU's\n",
    "            log_device_placement = False)) # shows GPU/CPU allocation\n",
    "         \n",
    "        start_time = time.time()\n",
    "        # Below is the code for running graph\n",
    "        sess.run(init)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoints_dir)\n",
    "        if ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print(\"Model %s restored\"%ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"Could not find any checkpoints at %s\"%FLAGS.checkpoints_dir)\n",
    "            return\n",
    "\n",
    "        tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "        out = sess.run(outputs, feed_dict = {x:test_ds})\n",
    "        duration = time.time()-start_time\n",
    "        print('Elapsed time: %.2f sec.' % (duration))\n",
    "        np.savetxt(FLAGS.output,out)\n",
    "        print('Outputs saved as %s'%FLAGS.output)\n",
    "        sess.close()\n",
    "    \n",
    "nn_outs, true_outs = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()#figsize=(8, 8))\n",
    "#point_num = 8\n",
    "#plt.plot(nn_outs[:1930,point_num],'b-',nn_outs[:1930,point_num+10],'bo',true_outs[:1930,point_num],'r-')\n",
    "hurricanes = range(0,193*10)\n",
    "for point_num in range(0,10):\n",
    "    ax = fig.add_subplot(10,1,point_num+1)\n",
    "    ax.plot(nn_outs[hurricanes,point_num],'b-',true_outs[hurricanes,point_num],'r-')\n",
    "#plt.plot(x_data,y_data[:,0],'ro',x_data, y_data[:,1],'bo',alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
