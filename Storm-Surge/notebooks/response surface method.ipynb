{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ann_dataset1.tar\n",
      "http://mrtee.europa.renci.org/~bblanton/ANN/ann_dataset1.tar\n",
      "Found and verified ann_dataset1.tar\n"
     ]
    }
   ],
   "source": [
    "# Download and save the archived data\n",
    "\n",
    "url = 'http://mrtee.europa.renci.org/~bblanton/ANN/'\n",
    "to = \"../data\"\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    print(os.path.join(to,filename))\n",
    "    print(url+filename)\n",
    "    if force or not os.path.exists(os.path.join(to,filename)):\n",
    "        filename, _ = urlretrieve(url + filename, os.path.join(to,filename))\n",
    "    statinfo = os.stat(os.path.join(to,filename))\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        raise Exception(\n",
    "          'Failed to verify' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "data_filename = maybe_download('ann_dataset1.tar', 5642240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data already present - Skipping extraction of ../data/ann_dataset_10points.tar.\n",
      "Processed 0/324 \n",
      "\n",
      "Processed 100/324 \n",
      "\n",
      "Processed 200/324 \n",
      "\n",
      "Processed 300/324 \n",
      "\n",
      "(324, 193, 18)\n"
     ]
    }
   ],
   "source": [
    "# Ten output data set\n",
    "# Extract files from the archive\n",
    "def maybe_extract(filename, force=False):\n",
    "    extract_folder = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    root = os.path.dirname(filename)\n",
    "    if os.path.isdir(extract_folder) and not force:\n",
    "    # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(path = root)\n",
    "        tar.close()\n",
    "    data_files = [\n",
    "        os.path.join(extract_folder, d) for d in sorted(os.listdir(extract_folder))\n",
    "        if os.path.isdir(extract_folder)]\n",
    "    return data_files\n",
    "  \n",
    "data_filename = \"../data/ann_dataset_10points.tar\"\n",
    "data_files = maybe_extract(data_filename)\n",
    "\n",
    "# Load files and produce dataset\n",
    "def maybe_load(file_names):\n",
    "    names = ('index','time', 'long', 'lat', 'param1', 'param2', 'param3', 'param4', 'out1', 'out2',\n",
    "            'out3', 'out4','out5', 'out6','out7', 'out8','out9', 'out10',)\n",
    "    datafile_length = 193\n",
    "    dataset = np.ndarray(shape=(len(file_names), datafile_length, len(names)))\n",
    "    for i in range(0,len(file_names)):\n",
    "        a = np.loadtxt(file_names[i])\n",
    "        a = np.asarray([x for xs in a for x in xs],dtype='d').reshape([datafile_length,len(names)])\n",
    "        dataset[i,:,:] = a\n",
    "        if i%100 == 0:\n",
    "            print(\"Processed %d/%d \\n\"%(i,len(file_names)))\n",
    "    return dataset\n",
    "\n",
    "dataset = maybe_load(data_files)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We need to convert the dataset into b(x) = [1 x1 x2 ... x6 x1^2 x1x2 x1x3 ... x6^2]\n",
    "\"\"\"\n",
    "def Normalize(x, means, stds):\n",
    "    return (x-means)/stds\n",
    "\n",
    "flat_dataset = np.reshape(dataset,(-1,18)) #\n",
    "inputs_only = flat_dataset[:,1:7]\n",
    "outputs_only = flat_dataset[:,8+9:9+9]\n",
    "\n",
    "input_means = [np.mean(inputs_only) for i in range(inputs_only.shape[1])]\n",
    "input_stds = [np.std(inputs_only) for i in range(inputs_only.shape[1])]\n",
    "inputs_only = (inputs_only-input_means)/input_stds\n",
    "\n",
    "inputs_quadratic = np.reshape(np.ones(inputs_only.shape[0]),(-1,1))\n",
    "inputs_quadratic = np.hstack((inputs_quadratic, inputs_only))\n",
    "\n",
    "for j in range(6):\n",
    "    for k in range(j,6):\n",
    "        ds = np.reshape(np.multiply(inputs_only[:,i],inputs_only[:,j]),(-1,1))\n",
    "        inputs_quadratic = np.hstack((inputs_quadratic, ds))\n",
    "\n",
    "test_start = 0 \n",
    "valid_start = 48*193 #int(test_percent/100.0*dataset.shape[0])\n",
    "train_start = (48+48)*193 #int((test_percent+valid_percent)/100.0*dataset.shape[0])\n",
    "\n",
    "test_dataset = np.hstack((inputs_quadratic[test_start:valid_start,:],outputs_only[test_start:valid_start,:]))\n",
    "valid_dataset = np.hstack((inputs_quadratic[valid_start:train_start,:],outputs_only[valid_start:train_start,:]))\n",
    "train_dataset = np.hstack((inputs_quadratic[train_start:,:],outputs_only[train_start:,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44004, 29)\n",
      "(44004, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "B = train_dataset\n",
    "x = test_dataset[234,1:7] # grab a random point from the test dataset\n",
    "d = train_dataset[:,1:7]-x\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 1001\n",
    "starter_learning_rate = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "\n",
    "    support_x = tf.placeholder(tf.float32, shape=(None, 28)) #set of support vectors, i.e. initially all training set\n",
    "    support_y_ = tf.placeholder(tf.float32, shape=(None, 1)) #set of support vectors outs\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=(None, 28)) #set of true vectors, i.e. initially all training set\n",
    "    y_ = tf.placeholder(tf.float32, shape=(None, 1)) #set of true vectors outs\n",
    "    \n",
    "    #d = \n",
    "    \"\"\"\n",
    "    # coefficients a\n",
    "    a_matrix = tf.Variable(tf.truncated_normal([28,1], stddev = 0.1, dtype = tf.float32))\n",
    "        \n",
    "    # calculate outputs at support vectors\n",
    "    support_y = tf.matmul(support_x,a_matrix)\n",
    "    \n",
    "    # support vectors weights\n",
    "    w_matrix = tf.Variable(tf.truncated_normal([batch_size,1], stddev = 0.1, dtype = tf.float32))\n",
    "    \n",
    "    # calculate losses at support vectors to optimize a_matrix and w_matrix\n",
    "    weighted_diff = tf.matmul(tf.transpose(tf.square(support_y-support_y_)),tf.abs(w_matrix))\n",
    "    loss = tf.reduce_mean(weighted_diff)\n",
    "    \n",
    "    global_step = tf.Variable(0.0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, num_steps, 1.0, staircase=False)\n",
    "    \n",
    "     # Create ADAM optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    # Calculate gradients and apply them\n",
    "    grads, v = zip(*optimizer.compute_gradients(loss))\n",
    "    grads, _ = tf.clip_by_global_norm(grads, 1.25)\n",
    "    apply_gradient_op = optimizer.apply_gradients(zip(grads,v), global_step = global_step)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0 (85.79 op/sec): 536.820801; validation loss: 0.000000\n",
      "Loss at step 50 (130.91 op/sec): 165.063232; validation loss: 0.000000\n",
      "Loss at step 100 (118.88 op/sec): 34.321072; validation loss: 0.000000\n",
      "Loss at step 150 (122.64 op/sec): 3.051094; validation loss: 0.000000\n",
      "Loss at step 200 (120.99 op/sec): 0.906050; validation loss: 0.000000\n",
      "Loss at step 250 (121.66 op/sec): 0.809061; validation loss: 0.000000\n",
      "Loss at step 300 (125.80 op/sec): 0.760943; validation loss: 0.000000\n",
      "Loss at step 350 (118.82 op/sec): 0.772881; validation loss: 0.000000\n",
      "Loss at step 400 (124.52 op/sec): 0.763551; validation loss: 0.000000\n",
      "Loss at step 450 (133.23 op/sec): 0.739514; validation loss: 0.000000\n",
      "Loss at step 500 (119.67 op/sec): 0.741241; validation loss: 0.000000\n",
      "Loss at step 550 (109.96 op/sec): 0.751168; validation loss: 0.000000\n",
      "Loss at step 600 (124.25 op/sec): 0.736728; validation loss: 0.000000\n",
      "Loss at step 650 (124.92 op/sec): 0.736312; validation loss: 0.000000\n",
      "Loss at step 700 (116.73 op/sec): 0.712569; validation loss: 0.000000\n",
      "Loss at step 750 (131.88 op/sec): 0.709970; validation loss: 0.000000\n",
      "Loss at step 800 (123.09 op/sec): 0.739903; validation loss: 0.000000\n",
      "Loss at step 850 (119.97 op/sec): 0.719569; validation loss: 0.000000\n",
      "Loss at step 900 (121.46 op/sec): 0.718824; validation loss: 0.000000\n",
      "Loss at step 950 (123.51 op/sec): 0.713745; validation loss: 0.000000\n",
      "Loss at step 1000 (127.16 op/sec): 0.709675; validation loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        #offset = (step * batch_size) % (train_output.shape[0] - batch_size)\n",
    "        #batch_data = train_dataset[offset:(offset + batch_size),:28].astype(np.float32)\n",
    "        #batch_output = train_dataset[offset:(offset + batch_size), 28:].astype(np.float32)\n",
    "        batch_data = train_dataset[:,:28].astype(np.float32)\n",
    "        batch_output = train_dataset[:, 28:].astype(np.float32)\n",
    "\n",
    "        feed_dict = {x : batch_data, y_ : batch_output}\n",
    "        start_time = time.time()\n",
    "        _, l = session.run([apply_gradient_op, loss],feed_dict=feed_dict)\n",
    "\n",
    "        duration = time.time()-start_time\n",
    "        \n",
    "        if (step % 50 == 0):\n",
    "            #valid_loss = loss.eval(feed_dict = {x: valid_dataset[:,:28], y_: valid_dataset[:,28:]})\n",
    "            valid_loss = 0\n",
    "            #print(predictions)\n",
    "            #ev = explained_variance_score(y_.eval(feed_dict = {x: valid_dataset2, y: valid_output}), valid_output)\n",
    "            #ev_l.append(valid_loss)\n",
    "            #ev_l = ev_l[1:]\n",
    "            print('Loss at step %d (%.2f op/sec): %f; validation loss: %.6f' % (\n",
    "                    step, 1.0/duration, l, \n",
    "                    #accuracy_mse(valid_prediction.eval(), valid_output)))\n",
    "                    valid_loss))\n",
    "            #if stop(ev_l):\n",
    "            #    print(\"Non decreasing scores, so stopping early\")\n",
    "            #    break\n",
    "    \n",
    "    #feed_dict = {x: test_dataset[:,:28],y_: test_dataset[:,28:]}\n",
    "    #predictions, test_loss = session.run([y, loss],feed_dict=feed_dict)\n",
    "    #test_loss = loss.eval(feed_dict = {x: test_dataset2, y: test_output})\n",
    "    #print('Test MSE: %.4f' % test_loss)\n",
    "    #print('Test losses:', test_losses)\n",
    "    #predicted_vs_actual = np.hstack((predictions, test_output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point 0: Max loss: 5.530039, MSE: 0.206465 CC: 0.208662  \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for axis 1 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-5ac3638afac9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mover_95\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_vs_actual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredicted_vs_actual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_vs_actual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpredicted_vs_actual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_vs_actual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpredicted_vs_actual\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 11 is out of bounds for axis 1 with size 11"
     ]
    }
   ],
   "source": [
    "over_95 = 0\n",
    "for i in range(10):\n",
    "    cc = np.corrcoef(predicted_vs_actual[:,i],predicted_vs_actual[:,i+10])[0,1]\n",
    "    m = np.max(np.abs(predicted_vs_actual[:,i]-predicted_vs_actual[:,i+10]))\n",
    "    mse = np.mean(np.sqrt(np.square(predicted_vs_actual[:,i]-predicted_vs_actual[:,i+10])))\n",
    "    \n",
    "    if cc >= 0.95:\n",
    "        over_95+=1\n",
    "    print('Point %d: Max loss: %.6f, MSE: %.6f CC: %.6f %c' % (i,m, mse, cc, 'v' if cc >= 0.95 else ' '))\n",
    "\n",
    "i = 5\n",
    "k = np.argmax(np.abs(predicted_vs_actual[:,i]-predicted_vs_actual[:,i+10]))\n",
    "print(k)\n",
    "max_error_case = [np.abs(predicted_vs_actual[k,i]-predicted_vs_actual[k,i+10]) for i in range(10)]\n",
    "print(max_error_case)\n",
    "start = (k // 193)*193\n",
    "stop = start + 193\n",
    "start = 0\n",
    "stop = 20*193\n",
    "\n",
    "print(predicted_vs_actual.shape)\n",
    "fig = plt.figure(figsize=(10, 6), dpi=80)\n",
    "\n",
    "for i in range(10):\n",
    "    sp = fig.add_subplot(10,1,i+10)\n",
    "    sp.plot(predicted_vs_actual[start:stop,i],color=\"blue\", linewidth=1.0, linestyle=\"-\", label=\"ANN\")\n",
    "    sp.plot(predicted_vs_actual[start:stop,i+10],color=\"red\", linewidth=1.0, linestyle=\"-\", label=\"actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  1.]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-231-59addae8a709>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-231-59addae8a709>\u001b[0m in \u001b[0;36mstop\u001b[1;34m(data, n)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mavg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d = np.asarray([])\n",
    "\n",
    "d = np.append(d,3)\n",
    "d = np.append(d,2)\n",
    "d = np.append(d,1)[1:]\n",
    "print(d)\n",
    "\n",
    "def stop(data, n = 2):\n",
    "    assert len(data) > n*2\n",
    "    avg = sum(data)/len(data)\n",
    "    for x in (data-avg)[-n:]:\n",
    "        if x <= 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "print(stop(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance:  [0.81146699190139771, 0.85759055614471436, 0.86870527267456055, 0.89061909914016724, 0.88695210218429565, 0.74401175975799561, 0.8059995174407959, 0.80061358213424683, 0.45739859342575073, 0.35861217975616455]\n",
      "(1.4245720507513746, -0.030474568797174348, 0.062562678013771522)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nnum_bins = 100\\n# the histogram of the errors\\nn, bins, patches = plt.hist(diff, num_bins, normed=1, facecolor='green', alpha=0.5)\\n\\n# add a normal PDF\\nmu = 0\\nsigma = .05\\ny = mlab.normpdf(bins, mu, sigma)\\nplt.plot(bins, y, 'r-')\\nplt.xlabel('Smarts')\\nplt.ylabel('Probability')\\n\\n# add Cauchy PDF\\nparams = cauchy.fit(diff)\\nprint(params)\\ndist = cauchy(params[0], params[1])\\nx = np.linspace(-2, 2, num_bins)\\nplt.plot(x, dist.pdf(x), 'b-', alpha=0.5, label='cauchy pdf')\\n\\n\\n# Tweak spacing to prevent clipping of ylabel\\n#plt.subplots_adjust(left=0.15)\\n#plt.show()\\n\\nfig = plt.figure(figsize=(10,6),dpi=80)\\nplt.hist(diff, bins = 100, alpha=0.5)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "import scipy.stats as stats\n",
    "import pylab\n",
    "\n",
    "ev = []\n",
    "for i in range(10):\n",
    "    y_true = predicted_vs_actual[:,i]\n",
    "    y_pred = predicted_vs_actual[:,i+10]\n",
    "    ev.append(explained_variance_score(y_true, y_pred))\n",
    "    \n",
    "print(\"Explained variance: \",ev)\n",
    "diff = y_true-y_pred\n",
    "\n",
    "#stats.probplot(diff, dist=\"norm\", plot=pylab)\n",
    "stats.probplot(diff, dist=\"t\", sparams=(2), plot=pylab)\n",
    "pylab.show()\n",
    "\n",
    "num_bins = 100\n",
    "# the histogram of the errors\n",
    "n, bins, patches = plt.hist(diff, num_bins, normed=1, facecolor='green', alpha=0.5)\n",
    "\n",
    "params = stats.t.fit(diff)\n",
    "dist = stats.t(params[0], params[1], params[2])\n",
    "x = np.linspace(-2, 2, num_bins)\n",
    "plt.plot(x, dist.pdf(x), 'b-', lw = 3, alpha=0.5, label='t pdf')\n",
    "plt.show()\n",
    "print(params)\n",
    "\n",
    "\"\"\"\n",
    "num_bins = 100\n",
    "# the histogram of the errors\n",
    "n, bins, patches = plt.hist(diff, num_bins, normed=1, facecolor='green', alpha=0.5)\n",
    "\n",
    "# add a normal PDF\n",
    "mu = 0\n",
    "sigma = .05\n",
    "y = mlab.normpdf(bins, mu, sigma)\n",
    "plt.plot(bins, y, 'r-')\n",
    "plt.xlabel('Smarts')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "# add Cauchy PDF\n",
    "params = cauchy.fit(diff)\n",
    "print(params)\n",
    "dist = cauchy(params[0], params[1])\n",
    "x = np.linspace(-2, 2, num_bins)\n",
    "plt.plot(x, dist.pdf(x), 'b-', alpha=0.5, label='cauchy pdf')\n",
    "\n",
    "\n",
    "# Tweak spacing to prevent clipping of ylabel\n",
    "#plt.subplots_adjust(left=0.15)\n",
    "#plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10,6),dpi=80)\n",
    "plt.hist(diff, bins = 100, alpha=0.5)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
