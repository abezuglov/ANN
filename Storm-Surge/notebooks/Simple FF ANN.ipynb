{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_float('learning_rate', 0.05, 'Initial learning rate')\n",
    "flags.DEFINE_float('learning_rate_decay', 0.1, 'Learning rate decay, i.e. the fraction of the initial learning rate at the end of training')\n",
    "\n",
    "flags.DEFINE_integer('max_steps', 1000, 'Number of steps to run trainer')\n",
    "flags.DEFINE_float('max_loss', 0.01, 'Maximally acceptable validation MSE')\n",
    "flags.DEFINE_integer('batch_size', 64*193, 'Batch size. Divides evenly into the dataset size of 193')\n",
    "flags.DEFINE_integer('hidden1', 35, 'Size of the first hidden layer')\n",
    "flags.DEFINE_integer('hidden2', 10, 'Size of the second hidden layer')\n",
    "flags.DEFINE_integer('output_vars', 1, 'Size of the output layer')\n",
    "flags.DEFINE_integer('input_vars', 6, 'Size of the input layer')\n",
    "#flags.DEFINE_string('train_dir', './data/', 'Directory to put the training data') # not currently used\n",
    "flags.DEFINE_string('checkpoints_dir', './checkpoints/two-layer/'+dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'Directory to store checkpoints')\n",
    "flags.DEFINE_string('summaries_dir','./logs/two-layer/'+dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),'Summaries directory')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NSAMPLE = 10000\n",
    "#x_data = np.float32(np.random.uniform(-10.5, 10.5, (1, NSAMPLE))).T\n",
    "x_data = np.float32(np.arange(-15.0,15.0,30.0/NSAMPLE)).T\n",
    "r_data = np.float32(np.random.normal(0,1.0, size=(NSAMPLE)))\n",
    "\n",
    "#y_data = 10.0*np.exp(-x_data*x_data/0.1)\n",
    "y_data = np.float32(np.sin(0.75*x_data)*7.0+x_data*0.5 + r_data*0.02)\n",
    "#y_data_2 = np.float32(np.sin(0.5*x_data)*3.0-x_data*0.5+r_data*1.0)\n",
    "#y_data = np.hstack((y_data_1, y_data_2))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "#plt.plot(x_data,y_data[:,0],'ro',x_data, y_data[:,1],'bo',alpha=0.3)\n",
    "plt.plot(x_data, y_data,'r-',alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NSAMPLE = 1000\n",
    "x_data = np.float32(np.arange(0,1,1.0/NSAMPLE))\n",
    "y_data = np.float32(x_data-x_data*x_data)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.plot(x_data, y_data,'r-',alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9993, 6) (9993, 1)\n"
     ]
    }
   ],
   "source": [
    "x = y_data\n",
    "#x = np.asarray(range(100))\n",
    "input_size = 7\n",
    "#data = np.zeros(shape=(x.shape[0]-size,input_size))\n",
    "data = np.zeros(shape=(x.shape[0]-input_size,input_size), dtype = np.float)\n",
    "for i in range(x.shape[0]-input_size):\n",
    "    for j in range(input_size):\n",
    "        data[i,j] = x[i+j]\n",
    "inputs = np.hsplit(data, np.array([6, 7]))[0]\n",
    "outputs = np.hsplit(data, np.array([6, 7]))[1]\n",
    "print(inputs.shape, outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (49 op/sec): Training MSE: 1.00000, Validation MSE: 42.18028\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-669140e2fe5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-669140e2fe5b>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNSAMPLE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0mstart_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNSAMPLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0msample_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstart_point\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bezuglov@ad.renci.org/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"\n",
    "    Returns TF weight variable with given shape. The weights are normally distributed with mean = 0, stddev = 0.1\n",
    "    shape -- shape of the variable, i.e. [4,5] matrix of 4x5\n",
    "    \"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"\n",
    "    Returns TF bias variable with given shape. The biases are initially at 0.1\n",
    "    shape -- shape of the variable, i.e. [4] -- vector of length 4\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"\n",
    "    Adds multiple summaries (statistics) for a TF variable\n",
    "    var -- TF variable\n",
    "    name -- variable name\n",
    "    \"\"\"\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.scalar_summary(name+'/mean', mean)\n",
    "    stddev = tf.reduce_mean(tf.reduce_sum(tf.square(var-mean)))\n",
    "    tf.scalar_summary(name+'/stddev', stddev)\n",
    "    _min = tf.reduce_min(var)\n",
    "    #tf.scalar_summary(name+'/min', _min)\n",
    "    _max = tf.reduce_max(var)\n",
    "    #tf.scalar_summary(name+'/max', _max)\n",
    "    tf.histogram_summary(name, var)\n",
    "\n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act = tf.tanh):\n",
    "    \"\"\"\n",
    "    Creates and returns NN layer\n",
    "    input_tensor -- TF tensor at layer input\n",
    "    input_dim -- size of layer input\n",
    "    output_dim -- size of layer output\n",
    "    layer_name -- name of the layer for summaries (statistics)\n",
    "    act -- nonlinear activation function\n",
    "    \"\"\"\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights, layer_name+'/weights')\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_variable([output_dim])\n",
    "            variable_summaries(biases, layer_name+'/biases')\n",
    "        with tf.name_scope('WX_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights)+biases\n",
    "            tf.histogram_summary(layer_name+'/pre_activations', preactivate)\n",
    "        if act is not None:\n",
    "            activations = act(preactivate, 'activation')\n",
    "        else:\n",
    "            activations = preactivate\n",
    "        tf.histogram_summary(layer_name+'/activations', activations)\n",
    "    return activations\n",
    "\n",
    "def run_training():\n",
    "    \"\"\"\n",
    "    Creates a NN and runs its training/running\n",
    "    \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.name_scope('input'):\n",
    "            x = tf.placeholder(tf.float32, [None, FLAGS.input_vars], name='x-input')\n",
    "            y_ = tf.placeholder(tf.float32, [None, FLAGS.output_vars], name = 'y-input')\n",
    "  \n",
    "        hidden_1 = nn_layer(x, FLAGS.input_vars, FLAGS.hidden1, 'layer1')\n",
    "        hidden_2 = nn_layer(hidden_1, FLAGS.hidden1, FLAGS.hidden2, 'layer2')\n",
    "        train_prediction = nn_layer(hidden_2, FLAGS.hidden2, FLAGS.output_vars, 'output', act = None)      \n",
    "        \n",
    "        with tf.name_scope('MSE'):\n",
    "            prediction_diff = train_prediction-y_\n",
    "            MSE = tf.cast(tf.reduce_mean(tf.reduce_mean(tf.square(prediction_diff))),tf.float32)\n",
    "            tf.scalar_summary('MSE', MSE)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            global_step = tf.Variable(0.00, trainable=False)\n",
    "            learning_rate = tf.train.exponential_decay(FLAGS.learning_rate, \n",
    "                                                       global_step, FLAGS.max_steps, \n",
    "                                                       FLAGS.learning_rate_decay, staircase=False)        \n",
    "            #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "            #optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "                MSE, global_step=global_step)\n",
    "                  \n",
    "        merged = tf.merge_all_summaries()\n",
    "        init = tf.initialize_all_variables()\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        train_writer = tf.train.SummaryWriter(FLAGS.summaries_dir+'/train')\n",
    "        test_writer = tf.train.SummaryWriter(FLAGS.summaries_dir+'/validation')\n",
    "        sess.run(init)\n",
    "        \n",
    "        train_loss = 1\n",
    "        valid_loss = 1\n",
    "        for step in xrange(10):#xrange(FLAGS.max_steps):\n",
    "            start_time = time.time()\n",
    "            if step%100 != 0:\n",
    "                # regular training\n",
    "                #feed_dict = fill_feed_dict(train_dataset, x, y_, train = True)\n",
    "                feed_dict = {x:inputs, y_:outputs}\n",
    "                _, train_loss, lr, summary = sess.run([optimizer, MSE, learning_rate, merged], feed_dict=feed_dict)\n",
    "                train_writer.add_summary(summary,step)\n",
    "            else:\n",
    "                # check model fit\n",
    "                feed_dict = feed_dict = {x:inputs, y_:outputs}\n",
    "                valid_loss, summary = sess.run([MSE, merged], feed_dict = feed_dict)\n",
    "                test_writer.add_summary(summary,step)\n",
    "                duration = time.time()-start_time\n",
    "                print('Step %d (%d op/sec): Training MSE: %.5f, Validation MSE: %.5f' % (step, 1/duration, train_loss, valid_loss))\n",
    "            \n",
    "        step_by_step = False\n",
    "        if step_by_step:\n",
    "            predictions = np.zeros(shape=[NSAMPLE, 1])\n",
    "            start_point = np.reshape(inputs[0,:],(1,6))\n",
    "            print(start_point)\n",
    "            for i in xrange(1,data.shape[0]):\n",
    "                prediction = sample_prediction.eval({sample_input: start_point})\n",
    "                #start_point = np.reshape(data[i,0],prediction[0,1],(1,2))\n",
    "                start_point[:,0] = data[i,0]\n",
    "                start_point[:,1] = prediction[0,1]\n",
    "                predictions[i] = prediction[0,1]\n",
    "        else:\n",
    "            predictions = np.zeros(shape=[NSAMPLE, ])\n",
    "            start_point = np.reshape(data[0,:],(1,2))\n",
    "            for step in range(NSAMPLE):\n",
    "                prediction = sample_prediction.eval({sample_input: start_point})\n",
    "                start_point = np.reshape(prediction[0,:],(1,2))\n",
    "                predictions[i] = prediction[0,1]\n",
    "            print('=' * 80)\n",
    "        \n",
    "        #feed_dict = fill_feed_dict(test_dataset, x, y_, train = False)\n",
    "        #test_loss, summary = sess.run([MSE, merged], feed_dict = feed_dict)\n",
    "        #print('Test MSE: %.5f' % (test_loss))\n",
    "        \n",
    "        #predicted_vs_actual = np.hstack((test_prediction.eval(session = sess), test_dataset.outputs))\n",
    "        #print(\"correlation coefficients: \")\n",
    "        #print(np.corrcoef(predicted_vs_actual[:,0],predicted_vs_actual[:,2]))\n",
    "        #print(np.corrcoef(predicted_vs_actual[:,1],predicted_vs_actual[:,3]))\n",
    "        sess.close()\n",
    "\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 46721.972656 learning rate: 0.020000\n",
      "Average loss at step 100: 29237.300283 learning rate: 0.015890\n",
      "Average loss at step 200: 8925.407236 learning rate: 0.012625\n",
      "Average loss at step 300: 5307.461790 learning rate: 0.010031\n",
      "Average loss at step 400: 4273.199182 learning rate: 0.007969\n",
      "Average loss at step 500: 3532.226587 learning rate: 0.006332\n",
      "Average loss at step 600: 3256.917402 learning rate: 0.005031\n",
      "Average loss at step 700: 3097.492437 learning rate: 0.003997\n",
      "Average loss at step 800: 2923.008850 learning rate: 0.003176\n",
      "Average loss at step 900: 2659.730233 learning rate: 0.002523\n",
      "Average loss at step 1000: 2644.401714 learning rate: 0.002005\n",
      "================================================================================\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings+1):\n",
    "            #this_x = np.reshape(this_x,(1, FIN_SIZE))\n",
    "            #print(\"i: \",i)\n",
    "            #print(batches[i].shape)\n",
    "            feed_dict[train_data[i]] = np.reshape(batches[i],(batch_size,2))\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            #if step % (summary_frequency * 10) == 0:\n",
    "                #print('=' * 80)\n",
    "                #prediction_list = []\n",
    "                #for _ in range(1000):\n",
    "                #    start_point = np.reshape(y_data[0],(1,1))\n",
    "                #    prediction = sample_prediction.eval({sample_input: start_point})\n",
    "                #    prediction_list.append(prediction[0][0])\n",
    "                #print('=' * 80)\n",
    "                \n",
    "    print('=' * 80)\n",
    "    \n",
    "    step_by_step = False\n",
    "    if step_by_step:\n",
    "        predictions = np.zeros(shape=[NSAMPLE, 1])\n",
    "        start_point = np.reshape(data[0,:],(1,2))\n",
    "        for i in xrange(1,data.shape[0]):\n",
    "            prediction = sample_prediction.eval({sample_input: start_point})\n",
    "            #start_point = np.reshape(data[i,0],prediction[0,1],(1,2))\n",
    "            start_point[:,0] = data[i,0]\n",
    "            start_point[:,1] = prediction[0,1]\n",
    "            predictions[i] = prediction[0,1]\n",
    "    else:\n",
    "        predictions = np.zeros(shape=[NSAMPLE, ])\n",
    "        start_point = np.reshape(data[0,:],(1,2))\n",
    "        for step in range(NSAMPLE):\n",
    "            prediction = sample_prediction.eval({sample_input: start_point})\n",
    "            start_point = np.reshape(prediction[0,:],(1,2))\n",
    "            predictions[i] = prediction[0,1]\n",
    "        print('=' * 80)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-72206fc53351>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(prediction_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#plt.plot(x_data,y_data,'ro', x_test,y_test[:,0],'bo',alpha=0.3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "#print(y_test)\n",
    "#print(prediction_list)\n",
    "plt.figure(figsize=(8, 8))\n",
    "#plt.plot(x_data,y_data,'ro', x_test,y_test[:,0],'bo',alpha=0.3)\n",
    "#plt.plot(x_data,y_data,'ro', x_test,y_test[:,0],'bo', x_test, y_test[:,1], 'b-', alpha=0.3)\n",
    "plt.plot(x_data[:],y_data[:], 'r-', x_data[:],predictions[:],'b-')#,x_test, y_test,'bo',alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
